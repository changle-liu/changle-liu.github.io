[{"content":"有点偷懒的一年，但还是有不少满意的作品：\n天马 b3 第二次折，改了翅膀羽毛的层数，马头不是很满意\n（1）原创设计：神谷哲史\n（2）难度系数：高\n（3）纸张类型：白色薄牛皮纸\n（4）纸张大小：45cm\n（5） 折叠时间：20h\n孔雀东南飞 非常有几何线条感的一个设计\n（1）原创设计：212moving\n（2）难度系数：高\n（3）纸张类型：油纸\n（4）纸张大小：40cm\n（5） 折叠时间：8h\n蓝鲸 （1）原创设计：忘记了\n（2）难度系数：低\n（3）纸张类型：越前和纸\n（4）纸张大小：28cm\n（5） 折叠时间：3h\n圣诞鹤 （1）原创设计：有泽悠河\n（2）难度系数：低\n（3）纸张类型：MUJI\n（4）纸张大小：15cm\n（5） 折叠时间：1h\n西伯利亚鼯鼠 （1）原创设计：胜田恭平\n（2）难度系数：低\n（3）纸张类型：MUJI\n（4）纸张大小：15cm\n（5） 折叠时间：1h\n摔倒.jpg （1）原创设计：SYN\n（2）难度系数：低\n（3）纸张类型：MUJI\n（4）纸张大小：15cm\n（5） 折叠时间：1h\n","date":"2025-01-06T00:00:00Z","image":"https://changle-liu.github.io/p/2024%E6%8A%98%E7%BA%B8%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/images/20250106_1216328542_hu2345349879268711663.png","permalink":"https://changle-liu.github.io/p/2024%E6%8A%98%E7%BA%B8%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/","title":"2024折纸年度总结"},{"content":"Attention 等经典算法手撕 手撕经典算法 #1 Attention篇\n方法论 递归 确定递归函数的参数和返回值 确定终止条件 确定每一层递归的逻辑 回溯 回溯就是穷举出所有可能，一般用来解决不分顺序的组合问题\n1 2 3 4 5 6 7 8 9 10 11 12 void backtracking(参数) { if (终止条件) { 存放结果; return; } for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) { 处理节点; backtracking(路径，选择列表); // 递归 回溯，撤销处理结果 } } 贪心 如何由局部最优推出全局最优\n动态规划 由前一个状态推导而来\n确定 dp 数组的下标及含义 递推公式 初始化 dp 数组 确定遍历顺序 举例推导 dp 数组 Python 常用指令 1 2 3 # 最大值 import sys max_v = sys.maxsize # 9223372036854775807 (2^63 - 1) 1 2 3 4 5 6 7 8 9 10 # 字符串排序，返回list sorted(string) # 字符串拼接 \u0026#34;sep\u0026#34;.join([\u0026#34;hhhh\u0026#34;, \u0026#34;iiii\u0026#34;, \u0026#34;jjjj\u0026#34;]) # hhhhsepiiiisepjjjj # 因此获得倒序字符串的指令是： \u0026#34;\u0026#34;.join(sorted(string)) # 获得ascii码 ord(\u0026#39;a\u0026#39;) # 获得对应字符 chr(65) 1 2 3 4 # 创建一个字典，当访问不存在的key时，默认创建并将value设为int类型 mp = collections.defaultdict(int) # defaultdict(\u0026lt;class \u0026#39;int\u0026#39;\u0026gt;, {\u0026#39;a\u0026#39;: 3, \u0026#39;b\u0026#39;: 4}) # 同样可创建list类型的 mp = defaultdict(list) # defaultdict(\u0026lt;class \u0026#39;list\u0026#39;\u0026gt;, {\u0026#39;a\u0026#39;: [1, 2], \u0026#39;b\u0026#39;: [3]}) 1 2 # 元组可作为哈希的key，list不能 mp[tuple(ls)] = 1 双指针 移动零到数组末尾 左右指针从最左侧为起点\n左指针指向处理过的非零序列末尾后的 0，右指针指向待处理序列的头部，两指针之间全部为 0\n若右指针指的不是零，则交换左右指针指向的值，将右指针的非零值放到做指针处\n二叉树 144 二叉树前序遍历（迭代） 维护一个栈。\n（将根节点入栈） 将父节点取出 将右节点放入，再将左节点放入（这样左节点会比右节点先取出） 重复（接下来会将左节点取出并重复操作） 直到栈中没有元素 94 二叉树的中序遍历 同样维护一个栈\n根节点作为当前节点 找到最左的节点，并再寻找途中依次将节点放入栈 将栈中的元素取出，该元素的左分支已全部遍历。将该元素放入 res，然后将右节点作为当前节点重复以上操作 145 二叉树的后序遍历 后序遍历顺序：左 - 右 - 中\n可以按中 - 右 - 左 顺序遍历后将结果反转\n二叉树的统一迭代遍历 *** 向栈中添加节点时，按反序添加。这样在出栈时才是正序。\n如前序遍历：\n出栈：中 -\u0026gt; 左 -\u0026gt; 右\n因此入栈顺序：右 -\u0026gt; 左 -\u0026gt; 中\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class Solution { public List\u0026lt;Integer\u0026gt; preorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; result = new LinkedList\u0026lt;\u0026gt;(); Stack\u0026lt;TreeNode\u0026gt; st = new Stack\u0026lt;\u0026gt;(); if (root != null) st.push(root); while (!st.empty()) { TreeNode node = st.peek(); // if (node != null) { st.pop(); // 将该节点弹出，避免重复操作，下面再将右中左节点添加到栈中 if (node.right!=null) st.push(node.right); // 添加右节点（空节点不入栈） if (node.left!=null) st.push(node.left); // 添加左节点（空节点不入栈） st.push(node); // 添加中节点 st.push(null); // 中节点访问过，但是还没有处理，加入空节点做为标记。 } else { // 只有遇到空节点的时候，才将下一个节点放进结果集 st.pop(); // 将空节点弹出 node = st.peek(); // 重新取出栈中元素 st.pop(); //不要忘了 result.add(node.val); // 加入到结果集 } } return result; } } class Solution { public List\u0026lt;Integer\u0026gt; inorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; result = new LinkedList\u0026lt;\u0026gt;(); Stack\u0026lt;TreeNode\u0026gt; st = new Stack\u0026lt;\u0026gt;(); if (root != null) st.push(root); while (!st.empty()) { TreeNode node = st.peek(); if (node != null) { st.pop(); // 将该节点弹出，避免重复操作，下面再将右中左节点添加到栈中 if (node.right!=null) st.push(node.right); // 添加右节点（空节点不入栈） st.push(node); // 添加中节点 st.push(null); // 中节点访问过，但是还没有处理，加入空节点做为标记。 if (node.left!=null) st.push(node.left); // 添加左节点（空节点不入栈） } else { // 只有遇到空节点的时候，才将下一个节点放进结果集 st.pop(); // 将空节点弹出 node = st.peek(); // 重新取出栈中元素 st.pop(); result.add(node.val); // 加入到结果集 } } return result; } } 二叉树层序遍历 102.二叉树的层序遍历 107.二叉树的层次遍历II 199.二叉树的右视图 637.二叉树的层平均值 429.N叉树的层序遍历 515.在每个树行中找最大值 116.填充每个节点的下一个右侧节点指针 117.填充每个节点的下一个右侧节点指针II 104.二叉树的最大深度 111.二叉树的最小深度 List result = [[0], [1, 2], [3, 4, 5, 6]]\n递归：递归函数的参数中把深度放进去，递归时获取的节点放入result中对应深度的数组中\n迭代：新建一个存储Node的队列，对队列中存入的的每个结点，将其依次取出读取值放入result，并将这些节点下层的结点放入队列。重复直到子节点为null。\n106 中后序遍历序列构造二叉树 *** 回溯 77 组合 给定两个整数 n 和 k，返回 1 \u0026hellip; n 中所有可能的 k 个数的组合。\n示例: 输入: n = 4, k = 2 输出: [ [2,4], [3,4], [2,3], [1,2], [1,3], [1,4], ]\n剪枝 取数是从左到右依次取的。\n每层在取下一个数时，不会考虑左边（即之前）已经取过的数。\n因此可能存在情况：\n靠后的数是没必要取的，因为就算之后的数全取了，也会出现数量不够的问题。\n这些分支便是需要剪掉的分支。\n47 全排列II 这道题有点意思\n创建一个boolean used[]数组来记录数字是否被使用过\n剪枝需要剪除同层已经使用过的数字的分支\n同层数字已被使用过的条件：i \u0026gt; 0 \u0026amp;\u0026amp; nums[i] == nums[i-1] \u0026amp;\u0026amp; used[i-1] == false\n在向下递归时只挑选没被使用过的数字，即used[i] == false\n","date":"2025-01-06T00:00:00Z","permalink":"https://changle-liu.github.io/p/leetcode%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/","title":"LeetCode刷题记录"},{"content":"LoRA 外的其他微调方法 Adapter类 PEFT技术通过在预训练模型的各层之间插入较小的神经网络模块，这些新增的神经模块被称为\u0026quot;适配器\u0026quot;，在进行下游任务的微调时，只需对适配器参数进行训练便能实现高效微调的目标。\n在此基础上衍生出了AdapterP、Parallel等高效微调技术；\nAdapter Tuning\n设计了Adapter 结构，将其嵌入 Transformer 的结构里面，在训练时，固定住原来预训练模型的参数不变，只对新增的 Adapter 结构进行微调。\n首先是一个 down-project 层将高维度特征映射到低维特征 然后过一个非线形层之后，再用一个 up-project 结构将低维特征映射回原来的高维特征 同时也设计了 skip-connection 结构，确保了在最差的情况下能够退化为identity（类似残差结构） 能够在只额外对增加的 3.6% 参数规模（相比原来预训练模型的参数量）的情况下取得和Full-Finetuning 接近的效果（GLUE指标在0.4%以内）。\n首次提出针对 BERT 的 PEFT微调方式，拉开了 PEFT 研究的序幕。\n遗留问题：增加了模型层数，引入了额外的推理延迟。\nPrefixTuning类 PEFT技术通过在模型的输入或隐层添加k个额外可训练的前缀标记，模型微调时只训练这些前缀参数便能实现高效微调的目标。\n在此基础上衍生出了P-Tuning、P-Tuningv2等高效微调技术；\nPrefix-Tuning\n该方法是在输入 token 之前构造一段任务相关的 virtual tokens 作为 Prefix，然后训练的时候只更新 Prefix 部分的参数，而 Transformer 中的其他部分参数固定。\n遗留问题：难于训练，且预留给 Prompt 的序列挤占了下游任务的输入序列空间，影响模型性能。\n遗留问题：Prompt Tuning和P-tuning这两种方法都是在预训练模型参数规模够足够大时，才能达到和Fine-tuning类似的效果，而参数规模较小时效果则很差，且在sequence tagging任务上表现都很差。\nLoRA 及 LoRA 变体 对于解决许多问题来说，我们希望在给定的下游任务上训练LLM，例如对句子进行分类或生成给定问题的答案。但是如果直接使用微调，这就需要要训练有数百万到数十亿个参数的大模型。\nLoRA提供了另一种训练方法，通过减少训练的参数数量，这种方法更快、更容易进行。LoRA引入了两个矩阵A和B， 如果参数W的原始矩阵的大小为d × d，则矩阵A和B的大小分别为d × r和r × d，其中r要小得多(通常小于100)。参数r称为秩。LoRA 使用两个矩阵的乘积来模拟模型的参数变化。\n如果使用秩为r=16的LoRA，则这些矩阵的形状为16 x d，这样就大大减少了需要训练的参数数量。\nLoRA的最大的优点是，与微调相比，训练的参数更少，但是却能获得与微调基本相当的性能。\nLoRA及其变体概述：LoRA, DoRA, AdaLoRA, Delta-LoRA-阿里云开发者社区\nLoRA 引入两个低秩矩阵 A(dr)、B(rd)，冻结其余参数，使得微调的参数量大大减少\nA 被初始化为均值为 0 的正态分布，B 初始化为全 0\nLoRA+ B 的学习率设置为远高于 A，使训练更加高效\nLoRA-FA A 在初始化后被冻结，只训练 B，使得参数减半\nLoRA-drop x 作为隐藏层状态，利用 BAx 计算 LoRA 适配器的重要性，如果重要性较小则将其进行冻结\nAdaLoRA 对不同位置的 LoRA 适配器分配不同的秩\nDoRA 将矩阵中的向量分解成大小和方向的乘积，获得更高的准确度\n","date":"2024-12-25T00:00:00Z","permalink":"https://changle-liu.github.io/p/lora%E5%BE%AE%E8%B0%83/","title":"LoRA微调"},{"content":"动手学深度学习v2\n课程链接：https://courses.d2l.ai/zh-v2/\n词嵌入 word2vec 词向量：单词的特征向量\n词嵌入：单词映射到实向量\n独热向量：容易构造，但不能表示出不同词之间的相似度\n自监督的 word2vec：将每个词映射到一个固定长度的向量，能更好地表达不同词之间的相似性和类比关系\n跳元模型 Skip-Gram 连续词袋 CBOW BERT 预训练 BERT:\n基于微调的 NLP，新任务只需增加一个简单的输出层 只有编码器的 Transformer 输入表示 由于缺少了解码器，目标句子没地方输入，于是将源句子和目标句子拼接后放入编码器 以开头，通过分隔符将句子分割 段嵌入：将不同的句子区分 位置嵌入：位置编码可学习 1 2 3 import torch from torch import nn from d2l import torch as d2l 1 2 3 4 5 6 7 8 9 def get_tokens_and_segments(tokens_a, tokens_b=None): \u0026#34;\u0026#34;\u0026#34;获取输入序列的词元及其片段索引\u0026#34;\u0026#34;\u0026#34; tokens = [\u0026#39;\u0026lt;cls\u0026gt;\u0026#39;] + tokens_a + [\u0026#39;\u0026lt;sep\u0026gt;\u0026#39;] # 0和1分别标记片段A和B segments = [0] * (len(tokens_a) + 2) if tokens_b is not None: tokens += tokens_b + [\u0026#39;\u0026lt;sep\u0026gt;\u0026#39;] segments += [1] * (len(tokens_b) + 1) return tokens, segments 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class BERTEncoder(nn.Module): \u0026#34;\u0026#34;\u0026#34;BERT编码器\u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len=1000, key_size=768, query_size=768, value_size=768, **kwargs): super(BERTEncoder, self).__init__(**kwargs) self.token_embedding = nn.Embedding(vocab_size, num_hiddens) self.segment_embedding = nn.Embedding(2, num_hiddens) self.blks = nn.Sequential() for i in range(num_layers): self.blks.add_module(f\u0026#34;{i}\u0026#34;, d2l.EncoderBlock( key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, True)) # 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数 self.pos_embedding = nn.Parameter(torch.randn(1, max_len, num_hiddens)) def forward(self, tokens, segments, valid_lens): # 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens） X = self.token_embedding(tokens) + self.segment_embedding(segments) X = X + self.pos_embedding.data[:, :X.shape[1], :] for blk in self.blks: X = blk(X, valid_lens) return X 预训练任务 掩蔽语言模型 Transformer 的编码器是双向的，但标准语言模型做预测时要求单向 预训练任务中随机选择 15% 的词元替换成 微调时不会出现，因此选择输入中替换： 80%：换成 10%：换成随机词元 10%：保持原有词元 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class MaskLM(nn.Module): \u0026#34;\u0026#34;\u0026#34;BERT的掩蔽语言模型任务\u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs): super(MaskLM, self).__init__(**kwargs) self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens), nn.ReLU(), nn.LayerNorm(num_hiddens), nn.Linear(num_hiddens, vocab_size)) def forward(self, X, pred_positions): num_pred_positions = pred_positions.shape[1] pred_positions = pred_positions.reshape(-1) batch_size = X.shape[0] batch_idx = torch.arange(0, batch_size) # 假设batch_size=2，num_pred_positions=3 # 那么batch_idx是np.array（[0,0,0,1,1,1]） batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions) masked_X = X[batch_idx, pred_positions] masked_X = masked_X.reshape((batch_size, num_pred_positions, -1)) mlm_Y_hat = self.mlp(masked_X) return mlm_Y_hat mlm = MaskLM(vocab_size, num_hiddens) mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]]) mlm_Y_hat = mlm(encoded_X, mlm_positions) mlm_Y_hat.shape # torch.Size([2, 3, 10000]) 下一句子预测 预测一个句子对中的两句子是否相邻 训练样本：50%选择相邻句子对、50%随机句子对 将对应的输出放到一个全连接层中做预测 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class NextSentencePred(nn.Module): \u0026#34;\u0026#34;\u0026#34;BERT的下一句预测任务\u0026#34;\u0026#34;\u0026#34; def __init__(self, num_inputs, **kwargs): super(NextSentencePred, self).__init__(**kwargs) self.output = nn.Linear(num_inputs, 2) def forward(self, X): # X的形状：(batchsize,num_hiddens) return self.output(X) encoded_X = torch.flatten(encoded_X, start_dim=1) # NSP的输入形状:(batchsize，num_hiddens) nsp = NextSentencePred(encoded_X.shape[-1]) nsp_Y_hat = nsp(encoded_X) nsp_Y_hat.shape # torch.Size([2, 2]) 整合代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #@save class BERTModel(nn.Module): \u0026#34;\u0026#34;\u0026#34;BERT模型\u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len=1000, key_size=768, query_size=768, value_size=768, hid_in_features=768, mlm_in_features=768, nsp_in_features=768): super(BERTModel, self).__init__() self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len=max_len, key_size=key_size, query_size=query_size, value_size=value_size) self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features) self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens), nn.Tanh()) self.nsp = NextSentencePred(nsp_in_features) def forward(self, tokens, segments, valid_lens=None, pred_positions=None): encoded_X = self.encoder(tokens, segments, valid_lens) if pred_positions is not None: mlm_Y_hat = self.mlm(encoded_X, pred_positions) else: mlm_Y_hat = None # 用于下一句预测的多层感知机分类器的隐藏层，0是“\u0026lt;cls\u0026gt;”标记的索引 nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :])) return encoded_X, mlm_Y_hat, nsp_Y_hat 用于预训练 BERT 的数据集 1 2 3 4 import os import random import torch from d2l import torch as d2l 1 2 3 4 5 6 7 8 9 10 11 12 13 d2l.DATA_HUB[\u0026#39;wikitext-2\u0026#39;] = ( \u0026#39;https://s3.amazonaws.com/research.metamind.io/wikitext/\u0026#39; \u0026#39;wikitext-2-v1.zip\u0026#39;, \u0026#39;3c914d17d80b1459be871a5039ac23e752a53cbe\u0026#39;) def _read_wiki(data_dir): file_name = os.path.join(data_dir, \u0026#39;wiki.train.tokens\u0026#39;) with open(file_name, \u0026#39;r\u0026#39;) as f: lines = f.readlines() # 大写字母转换为小写字母 paragraphs = [line.strip().lower().split(\u0026#39; . \u0026#39;) for line in lines if len(line.split(\u0026#39; . \u0026#39;)) \u0026gt;= 2] random.shuffle(paragraphs) return paragraphs 为预训练任务定义辅助函数 生成下一句预测任务的数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def _get_next_sentence(sentence, next_sentence, paragraphs): # 50%概率直接返回 if random.random() \u0026lt; 0.5: is_next = True # 另50%概率将第二句替换成随机句子 else: # paragraphs是三重列表的嵌套 next_sentence = random.choice(random.choice(paragraphs)) is_next = False return sentence, next_sentence, is_next def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len): nsp_data_from_paragraph = [] for i in range(len(paragraph) - 1): tokens_a, tokens_b, is_next = _get_next_sentence( paragraph[i], paragraph[i + 1], paragraphs) # 考虑1个\u0026#39;\u0026lt;cls\u0026gt;\u0026#39;词元和2个\u0026#39;\u0026lt;sep\u0026gt;\u0026#39;词元 if len(tokens_a) + len(tokens_b) + 3 \u0026gt; max_len: continue tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b) nsp_data_from_paragraph.append((tokens, segments, is_next)) return nsp_data_from_paragraph 生成遮蔽语言模型任务的数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab): # 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“\u0026lt;mask\u0026gt;”或随机词元 mlm_input_tokens = [token for token in tokens] pred_positions_and_labels = [] # 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测 random.shuffle(candidate_pred_positions) for mlm_pred_position in candidate_pred_positions: if len(pred_positions_and_labels) \u0026gt;= num_mlm_preds: break masked_token = None # 80%的时间：将词替换为“\u0026lt;mask\u0026gt;”词元 if random.random() \u0026lt; 0.8: masked_token = \u0026#39;\u0026lt;mask\u0026gt;\u0026#39; else: # 10%的时间：保持词不变 if random.random() \u0026lt; 0.5: masked_token = tokens[mlm_pred_position] # 10%的时间：用随机词替换该词 else: masked_token = random.choice(vocab.idx_to_token) mlm_input_tokens[mlm_pred_position] = masked_token pred_positions_and_labels.append( (mlm_pred_position, tokens[mlm_pred_position])) return mlm_input_tokens, pred_positions_and_labels 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #@save def _get_mlm_data_from_tokens(tokens, vocab): candidate_pred_positions = [] # tokens是一个字符串列表 for i, token in enumerate(tokens): # 在遮蔽语言模型任务中不会预测特殊词元 if token in [\u0026#39;\u0026lt;cls\u0026gt;\u0026#39;, \u0026#39;\u0026lt;sep\u0026gt;\u0026#39;]: continue candidate_pred_positions.append(i) # 遮蔽语言模型任务中预测15%的随机词元 num_mlm_preds = max(1, round(len(tokens) * 0.15)) mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens( tokens, candidate_pred_positions, num_mlm_preds, vocab) pred_positions_and_labels = sorted(pred_positions_and_labels, key=lambda x: x[0]) pred_positions = [v[0] for v in pred_positions_and_labels] mlm_pred_labels = [v[1] for v in pred_positions_and_labels] return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels] 将文本转换为预训练数据集 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #@save def _pad_bert_inputs(examples, max_len, vocab): max_num_mlm_preds = round(max_len * 0.15) all_token_ids, all_segments, valid_lens, = [], [], [] all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], [] nsp_labels = [] for (token_ids, pred_positions, mlm_pred_label_ids, segments, is_next) in examples: all_token_ids.append(torch.tensor(token_ids + [vocab[\u0026#39;\u0026lt;pad\u0026gt;\u0026#39;]] * ( max_len - len(token_ids)), dtype=torch.long)) all_segments.append(torch.tensor(segments + [0] * ( max_len - len(segments)), dtype=torch.long)) # valid_lens不包括\u0026#39;\u0026lt;pad\u0026gt;\u0026#39;的计数 valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32)) all_pred_positions.append(torch.tensor(pred_positions + [0] * ( max_num_mlm_preds - len(pred_positions)), dtype=torch.long)) # 填充词元的预测将通过乘以0权重在损失中过滤掉 all_mlm_weights.append( torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * ( max_num_mlm_preds - len(pred_positions)), dtype=torch.float32)) all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * ( max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long)) nsp_labels.append(torch.tensor(is_next, dtype=torch.long)) return (all_token_ids, all_segments, valid_lens, all_pred_positions, all_mlm_weights, all_mlm_labels, nsp_labels) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #@save class _WikiTextDataset(torch.utils.data.Dataset): def __init__(self, paragraphs, max_len): # 输入paragraphs[i]是代表段落的句子字符串列表； # 而输出paragraphs[i]是代表段落的句子列表，其中每个句子都是词元列表 paragraphs = [d2l.tokenize( paragraph, token=\u0026#39;word\u0026#39;) for paragraph in paragraphs] sentences = [sentence for paragraph in paragraphs for sentence in paragraph] self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[ \u0026#39;\u0026lt;pad\u0026gt;\u0026#39;, \u0026#39;\u0026lt;mask\u0026gt;\u0026#39;, \u0026#39;\u0026lt;cls\u0026gt;\u0026#39;, \u0026#39;\u0026lt;sep\u0026gt;\u0026#39;]) # 获取下一句子预测任务的数据 examples = [] for paragraph in paragraphs: examples.extend(_get_nsp_data_from_paragraph( paragraph, paragraphs, self.vocab, max_len)) # 获取遮蔽语言模型任务的数据 examples = [(_get_mlm_data_from_tokens(tokens, self.vocab) + (segments, is_next)) for tokens, segments, is_next in examples] # 填充输入 (self.all_token_ids, self.all_segments, self.valid_lens, self.all_pred_positions, self.all_mlm_weights, self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs( examples, max_len, self.vocab) def __getitem__(self, idx): return (self.all_token_ids[idx], self.all_segments[idx], self.valid_lens[idx], self.all_pred_positions[idx], self.all_mlm_weights[idx], self.all_mlm_labels[idx], self.nsp_labels[idx]) def __len__(self): return len(self.all_token_ids) 1 2 3 4 5 6 7 8 9 10 #@save def load_data_wiki(batch_size, max_len): \u0026#34;\u0026#34;\u0026#34;加载WikiText-2数据集\u0026#34;\u0026#34;\u0026#34; num_workers = d2l.get_dataloader_workers() data_dir = d2l.download_extract(\u0026#39;wikitext-2\u0026#39;, \u0026#39;wikitext-2\u0026#39;) paragraphs = _read_wiki(data_dir) train_set = _WikiTextDataset(paragraphs, max_len) train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers) return train_iter, train_set.vocab 1 2 3 4 5 6 7 8 9 10 batch_size, max_len = 512, 64 train_iter, vocab = load_data_wiki(batch_size, max_len) for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y) in train_iter: print(tokens_X.shape, segments_X.shape, valid_lens_x.shape, pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape, nsp_y.shape) break # torch.Size([512, 64]) torch.Size([512, 64]) torch.Size([512]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512]) 预训练 BERT 14.10. 预训练BERT — 动手学深度学习 2.0.0 documentation (d2l.ai)\nBERT预训练代码_哔哩哔哩_bilibili\n情感分析 数据集 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import os import torch from torch import nn from d2l import torch as d2l d2l.DATA_HUB[\u0026#39;aclImdb\u0026#39;] = ( \u0026#39;http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\u0026#39;, \u0026#39;01ada507287d82875905620988597833ad4e0903\u0026#39;) data_dir = d2l.download_extract(\u0026#39;aclImdb\u0026#39;, \u0026#39;aclImdb\u0026#39;) def read_imdb(data_dir, is_train): \u0026#34;\u0026#34;\u0026#34;读取IMDb评论数据集文本序列和标签\u0026#34;\u0026#34;\u0026#34; data, labels = [], [] for label in (\u0026#39;pos\u0026#39;, \u0026#39;neg\u0026#39;): folder_name = os.path.join(data_dir, \u0026#39;train\u0026#39; if is_train else \u0026#39;test\u0026#39;, label) for file in os.listdir(folder_name): with open(os.path.join(folder_name, file), \u0026#39;rb\u0026#39;) as f: review = f.read().decode(\u0026#39;utf-8\u0026#39;).replace(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;) data.append(review) labels.append(1 if label == \u0026#39;pos\u0026#39; else 0) return data, labels train_data = read_imdb(data_dir, is_train=True) print(\u0026#39;训练集数目：\u0026#39;, len(train_data[0])) for x, y in zip(train_data[0][:3], train_data[1][:3]): print(\u0026#39;标签：\u0026#39;, y, \u0026#39;review:\u0026#39;, x[0:60]) # 训练集数目： 25000 # 标签： 1 review: Bromwell High is a cartoon comedy. It ran at the same time a # 标签： 1 review: Homelessness (or Houselessness as George Carlin stated) has # 标签： 1 review: Brilliant over-acting by Lesley Ann Warren. Best dramatic ho train_tokens = d2l.tokenize(train_data[0], token=\u0026#39;word\u0026#39;) vocab = d2l.Vocab(train_tokens, min_freq=5, reserved_tokens=[\u0026#39;\u0026lt;pad\u0026gt;\u0026#39;]) d2l.set_figsize() d2l.plt.xlabel(\u0026#39;# tokens per review\u0026#39;) d2l.plt.ylabel(\u0026#39;count\u0026#39;) d2l.plt.hist([len(line) for line in train_tokens], bins=range(0, 1000, 50)); 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 num_steps = 500 # 序列长度 train_features = torch.tensor([d2l.truncate_pad( vocab[line], num_steps, vocab[\u0026#39;\u0026lt;pad\u0026gt;\u0026#39;]) for line in train_tokens]) print(train_features.shape) # print(train_features.shape) train_iter = d2l.load_array((train_features, torch.tensor(train_data[1])), 64) for X, y in train_iter: print(\u0026#39;X:\u0026#39;, X.shape, \u0026#39;, y:\u0026#39;, y.shape) break print(\u0026#39;小批量数目：\u0026#39;, len(train_iter)) # X: torch.Size([64, 500]) , y: torch.Size([64]) # 小批量数目： 391 def load_data_imdb(batch_size, num_steps=500): \u0026#34;\u0026#34;\u0026#34;返回数据迭代器和IMDb评论数据集的词表\u0026#34;\u0026#34;\u0026#34; data_dir = d2l.download_extract(\u0026#39;aclImdb\u0026#39;, \u0026#39;aclImdb\u0026#39;) train_data = read_imdb(data_dir, True) test_data = read_imdb(data_dir, False) train_tokens = d2l.tokenize(train_data[0], token=\u0026#39;word\u0026#39;) test_tokens = d2l.tokenize(test_data[0], token=\u0026#39;word\u0026#39;) vocab = d2l.Vocab(train_tokens, min_freq=5) train_features = torch.tensor([d2l.truncate_pad( vocab[line], num_steps, vocab[\u0026#39;\u0026lt;pad\u0026gt;\u0026#39;]) for line in train_tokens]) test_features = torch.tensor([d2l.truncate_pad( vocab[line], num_steps, vocab[\u0026#39;\u0026lt;pad\u0026gt;\u0026#39;]) for line in test_tokens]) train_iter = d2l.load_array((train_features, torch.tensor(train_data[1])), batch_size) test_iter = d2l.load_array((test_features, torch.tensor(test_data[1])), batch_size, is_train=False) return train_iter, test_iter, vocab 使用循环神经网络 序列级和词元级使用微调 BERT BERT 会对每一个词元返回抽取了上下文信息的特征向量 即使下游的任务各有不同，微调时只需要加输出层，并使用相应的 BERT 特征 单文本分类 将对应的向量输入到全连接层分类 命名实体识别 识别一个词元是否是命名实体，如人名、机构、位置 将非特殊词元放进全连接分类 问题回答 给定问题、描述文字（分隔），找出片段作为回答 对片段中的每个词元，预测其是否是回答的开头或结束 自然语言推理数据集 15.4. 自然语言推断与数据集 — 动手学深度学习 2.0.0 documentation (d2l.ai)\n1 2 3 4 5 6 7 8 9 10 11 12 import os import re import torch from torch import nn from d2l import torch as d2l #@save d2l.DATA_HUB[\u0026#39;SNLI\u0026#39;] = ( \u0026#39;https://nlp.stanford.edu/projects/snli/snli_1.0.zip\u0026#39;, \u0026#39;9fcde07509c7e87ec61c640c1b2753d9041758e4\u0026#39;) data_dir = d2l.download_extract(\u0026#39;SNLI\u0026#39;) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def read_snli(data_dir, is_train): \u0026#34;\u0026#34;\u0026#34;将SNLI数据集解析为前提、假设和标签\u0026#34;\u0026#34;\u0026#34; def extract_text(s): # 删除我们不会使用的信息 s = re.sub(\u0026#39;\\\\(\u0026#39;, \u0026#39;\u0026#39;, s) s = re.sub(\u0026#39;\\\\)\u0026#39;, \u0026#39;\u0026#39;, s) # 用一个空格替换两个或多个连续的空格 s = re.sub(\u0026#39;\\\\s{2,}\u0026#39;, \u0026#39; \u0026#39;, s) return s.strip() label_set = {\u0026#39;entailment\u0026#39;: 0, \u0026#39;contradiction\u0026#39;: 1, \u0026#39;neutral\u0026#39;: 2} file_name = os.path.join(data_dir, \u0026#39;snli_1.0_train.txt\u0026#39; if is_train else \u0026#39;snli_1.0_test.txt\u0026#39;) with open(file_name, \u0026#39;r\u0026#39;) as f: rows = [row.split(\u0026#39;\\t\u0026#39;) for row in f.readlines()[1:]] premises = [extract_text(row[1]) for row in rows if row[0] in label_set] hypotheses = [extract_text(row[2]) for row in rows if row[0] \\ in label_set] labels = [label_set[row[0]] for row in rows if row[0] in label_set] return premises, hypotheses, labels 1 2 3 4 5 6 7 8 9 10 11 12 13 14 train_data = read_snli(data_dir, is_train=True) for x0, x1, y in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]): print(\u0026#39;前提：\u0026#39;, x0) print(\u0026#39;假设：\u0026#39;, x1) print(\u0026#39;标签：\u0026#39;, y) # 前提： A person on a horse jumps over a broken down airplane . # 假设： A person is training his horse for a competition . # 标签： 2 # 前提： A person on a horse jumps over a broken down airplane . # 假设： A person is at a diner , ordering an omelette . # 标签： 1 # 前提： A person on a horse jumps over a broken down airplane . # 假设： A person is outdoors , on a horse . # 标签： 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class SNLIDataset(torch.utils.data.Dataset): \u0026#34;\u0026#34;\u0026#34;用于加载SNLI数据集的自定义数据集\u0026#34;\u0026#34;\u0026#34; def __init__(self, dataset, num_steps, vocab=None): self.num_steps = num_steps all_premise_tokens = d2l.tokenize(dataset[0]) all_hypothesis_tokens = d2l.tokenize(dataset[1]) if vocab is None: self.vocab = d2l.Vocab(all_premise_tokens + \\ all_hypothesis_tokens, min_freq=5, reserved_tokens=[\u0026#39;\u0026lt;pad\u0026gt;\u0026#39;]) else: self.vocab = vocab self.premises = self._pad(all_premise_tokens) self.hypotheses = self._pad(all_hypothesis_tokens) self.labels = torch.tensor(dataset[2]) print(\u0026#39;read \u0026#39; + str(len(self.premises)) + \u0026#39; examples\u0026#39;) def _pad(self, lines): return torch.tensor([d2l.truncate_pad( self.vocab[line], self.num_steps, self.vocab[\u0026#39;\u0026lt;pad\u0026gt;\u0026#39;]) for line in lines]) def __getitem__(self, idx): return (self.premises[idx], self.hypotheses[idx]), self.labels[idx] def __len__(self): return len(self.premises) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def load_data_snli(batch_size, num_steps=50): \u0026#34;\u0026#34;\u0026#34;下载SNLI数据集并返回数据迭代器和词表\u0026#34;\u0026#34;\u0026#34; num_workers = d2l.get_dataloader_workers() data_dir = d2l.download_extract(\u0026#39;SNLI\u0026#39;) train_data = read_snli(data_dir, True) test_data = read_snli(data_dir, False) train_set = SNLIDataset(train_data, num_steps) test_set = SNLIDataset(test_data, num_steps, train_set.vocab) train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers) test_iter = torch.utils.data.DataLoader(test_set, batch_size, shuffle=False, num_workers=num_workers) return train_iter, test_iter, train_set.vocab 自然语言推断：微调 BERT 1 2 3 4 5 6 import json import multiprocessing import os import torch from torch import nn from d2l import torch as d2l 加载预训练的 BERT\n1 2 3 4 d2l.DATA_HUB[\u0026#39;bert.base\u0026#39;] = (d2l.DATA_URL + \u0026#39;bert.base.torch.zip\u0026#39;, \u0026#39;225d66f04cae318b841a13d32af3acc165f253ac\u0026#39;) d2l.DATA_HUB[\u0026#39;bert.small\u0026#39;] = (d2l.DATA_URL + \u0026#39;bert.small.torch.zip\u0026#39;, \u0026#39;c72329e68a732bef0452e4b96a1c341c8910f81f\u0026#39;) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens, num_heads, num_layers, dropout, max_len, devices): data_dir = d2l.download_extract(pretrained_model) # 定义空词表以加载预定义词表 vocab = d2l.Vocab() vocab.idx_to_token = json.load(open(os.path.join(data_dir, \u0026#39;vocab.json\u0026#39;))) vocab.token_to_idx = {token: idx for idx, token in enumerate( vocab.idx_to_token)} bert = d2l.BERTModel(len(vocab), num_hiddens, norm_shape=[256], ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens, num_heads=4, num_layers=2, dropout=0.2, max_len=max_len, key_size=256, query_size=256, value_size=256, hid_in_features=256, mlm_in_features=256, nsp_in_features=256) # 加载预训练BERT参数 bert.load_state_dict(torch.load(os.path.join(data_dir, \u0026#39;pretrained.params\u0026#39;))) return bert, vocab devices = d2l.try_all_gpus() bert, vocab = load_pretrained_model( \u0026#39;bert.small\u0026#39;, num_hiddens=256, ffn_num_hiddens=512, num_heads=4, num_layers=2, dropout=0.1, max_len=512, devices=devices) 微调 BERT 的数据集\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class SNLIBERTDataset(torch.utils.data.Dataset): def __init__(self, dataset, max_len, vocab=None): all_premise_hypothesis_tokens = [[ p_tokens, h_tokens] for p_tokens, h_tokens in zip( *[d2l.tokenize([s.lower() for s in sentences]) for sentences in dataset[:2]])] self.labels = torch.tensor(dataset[2]) self.vocab = vocab self.max_len = max_len (self.all_token_ids, self.all_segments, self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens) print(\u0026#39;read \u0026#39; + str(len(self.all_token_ids)) + \u0026#39; examples\u0026#39;) def _preprocess(self, all_premise_hypothesis_tokens): pool = multiprocessing.Pool(4) # 使用4个进程 out = pool.map(self._mp_worker, all_premise_hypothesis_tokens) all_token_ids = [ token_ids for token_ids, segments, valid_len in out] all_segments = [segments for token_ids, segments, valid_len in out] valid_lens = [valid_len for token_ids, segments, valid_len in out] return (torch.tensor(all_token_ids, dtype=torch.long), torch.tensor(all_segments, dtype=torch.long), torch.tensor(valid_lens)) def _mp_worker(self, premise_hypothesis_tokens): p_tokens, h_tokens = premise_hypothesis_tokens self._truncate_pair_of_tokens(p_tokens, h_tokens) tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens) token_ids = self.vocab[tokens] + [self.vocab[\u0026#39;\u0026lt;pad\u0026gt;\u0026#39;]] \\ * (self.max_len - len(tokens)) segments = segments + [0] * (self.max_len - len(segments)) valid_len = len(tokens) return token_ids, segments, valid_len def _truncate_pair_of_tokens(self, p_tokens, h_tokens): # 为BERT输入中的\u0026#39;\u0026lt;CLS\u0026gt;\u0026#39;、\u0026#39;\u0026lt;SEP\u0026gt;\u0026#39;和\u0026#39;\u0026lt;SEP\u0026gt;\u0026#39;词元保留位置 while len(p_tokens) + len(h_tokens) \u0026gt; self.max_len - 3: if len(p_tokens) \u0026gt; len(h_tokens): p_tokens.pop() else: h_tokens.pop() def __getitem__(self, idx): return (self.all_token_ids[idx], self.all_segments[idx], self.valid_lens[idx]), self.labels[idx] def __len__(self): return len(self.all_token_ids) # 如果出现显存不足错误，请减少“batch_size”。在原始的BERT模型中，max_len=512 batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers() data_dir = d2l.download_extract(\u0026#39;SNLI\u0026#39;) train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab) test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab) train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers) test_iter = torch.utils.data.DataLoader(test_set, batch_size, num_workers=num_workers) # read 549367 examples # read 9824 examples 微调 BERT\n1 2 3 4 5 6 7 8 9 10 11 12 13 class BERTClassifier(nn.Module): def __init__(self, bert): super(BERTClassifier, self).__init__() self.encoder = bert.encoder self.hidden = bert.hidden self.output = nn.Linear(256, 3) def forward(self, inputs): tokens_X, segments_X, valid_lens_x = inputs encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x) return self.output(self.hidden(encoded_X[:, 0, :])) net = BERTClassifier(bert) 微调 tricks 处理长文本： 分为截断法和层级法\n截断法：头、尾、头+尾\n层级法：分成 k 个片段，分别喂给 BERT 后将表示向量通过均值池化等方式来组合所有分数的表示\n不同层的特征：最后一层表征效果最好，最后四层进行最大值池化效果最好 灾难性遗忘 Catastrophic forgetting (灾难性遗忘)通常是迁移学习中的常见诟病，这意味着在学习新知识的过程中预先训练的知识会被遗忘。\n2e-5 才能克服灾难性遗忘问题，预训练模型训练不能收敛时要多检查超参数设置是否有问题\nITPT：继续预训练 Bert是在通用的语料上进行预训练的，如果要在特定领域应用文本分类，数据分布一定是有一些差距的。这时候可以考虑进行深度预训练。\nWithin-task pre-training：Bert在训练语料上进行预训练 In-domain pre-training：在同一领域上的语料进行预训练\nCross-domain pre-training：在不同领域上的语料进行预训练\nBERT 在 Adam 中移除了偏差纠正，在微调时尽量使用完整版 Adam 1 optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8) 尽量训练 3 个以上 epoch 可以固定住底层或将顶层随机初始化 ","date":"2024-10-27T00:00:00Z","permalink":"https://changle-liu.github.io/p/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","title":"自然语言处理基础-动手学深度学习"},{"content":"动手学深度学习v2\n课程链接：https://courses.d2l.ai/zh-v2/\n优化问题 一般形式：minimize f(x) subject to x belongs to C\n全局最小 - 局部最小\n凸集\n凸函数\n凸优化问题：f 凸函数，C 凸集，则局部最小便是全局最小\n梯度下降：求多个样本的平均梯度\n随机梯度下降：随机选取样本求梯度\n小批量随机梯度下降 sgd：采样一个随机子集求梯度\n冲量法：\nAdam：对学习率不敏感\nt 比较小时进行修正\nt 较大时同样进行修正\n","date":"2024-09-29T00:00:00Z","permalink":"https://changle-liu.github.io/p/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","title":"优化算法-动手学深度学习"},{"content":"动手学深度学习v2\n课程链接：https://courses.d2l.ai/zh-v2/\n注意力提示 主要思想 随意线索：主动有意识去观察线索\n随意线索：查询（query） 感觉输入（value） 不随意线索（key） 注意力池化层/汇聚层 query(value) -\u0026gt; key\n非参注意力池化层 平均池化：\nNadaraya-Watson 核回归：\n使用高斯核：\n则\nf(x) 可写作，其中$ \\alpha(x, x_i) $为权重\nNadaraya-Watson 核回归代码实现 1 2 3 import torch from torch import nn from d2l import torch as d2l 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 n_train = 50 # 训练样本数 x_train, _ = torch.sort(torch.rand(n_train) * 5) # 排序后的训练样本 # 真实f(x) def f(x): return 2 * torch.sin(x) + x**0.8 y_train = f(x_train) + torch.normal(0.0, 0.5, (n_train,)) # 训练样本的输出 x_test = torch.arange(0, 5, 0.1) # 测试样本 y_truth = f(x_test) # 测试样本的真实输出 n_test = len(x_test) # 测试样本数 n_test # 50 def plot_kernel_reg(y_hat): d2l.plot(x_test, [y_truth, y_hat], \u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;, legend=[\u0026#39;Truth\u0026#39;, \u0026#39;Pred\u0026#39;], xlim=[0, 5], ylim=[-1, 5]) d2l.plt.plot(x_train, y_train, \u0026#39;o\u0026#39;, alpha=0.5); 平均汇聚\n1 2 y_hat = torch.repeat_interleave(y_train.mean(), n_test) plot_kernel_reg(y_hat) 非参数注意力汇聚\n1 2 3 4 5 6 7 8 9 # X_repeat的形状:(n_test,n_train), # 每一行都包含着相同的测试输入（例如：同样的查询） X_repeat = x_test.repeat_interleave(n_train).reshape((-1, n_train)) # x_train包含着键。attention_weights的形状：(n_test,n_train), # 每一行都包含着要在给定的每个查询的值（y_train）之间分配的注意力权重 attention_weights = nn.functional.softmax(-(X_repeat - x_train)**2 / 2, dim=1) # y_hat的每个元素都是值的加权平均值，其中的权重是注意力权重 y_hat = torch.matmul(attention_weights, y_train) plot_kernel_reg(y_hat) 1 2 3 d2l.show_heatmaps(attention_weights.unsqueeze(0).unsqueeze(0), xlabel=\u0026#39;Sorted training inputs\u0026#39;, ylabel=\u0026#39;Sorted testing inputs\u0026#39;) 带参数的注意力汇聚\n1 2 3 4 5 6 7 8 9 10 11 12 13 # batch_size = 2, (1,4)和(4,6)的矩阵做乘法 X = torch.ones((2, 1, 4)) Y = torch.ones((2, 4, 6)) torch.bmm(X, Y).shape # torch.Size([2, 1, 6]) # 在注意力机制的背景中，我们可以[使用小批量矩阵乘法来计算小批量数据中的加权平均值] weights = torch.ones((2, 10)) * 0.1 values = torch.arange(20.0).reshape((2, 10)) torch.bmm(weights.unsqueeze(1), values.unsqueeze(-1)) # tensor([[[ 4.5000]], # [[14.5000]]]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class NWKernelRegression(nn.Module): def __init__(self, **kwargs): super().__init__(**kwargs) self.w = nn.Parameter(torch.rand((1,), requires_grad=True)) def forward(self, queries, keys, values): # queries和attention_weights的形状为(查询个数，“键－值”对个数) queries = queries.repeat_interleave(keys.shape[1]).reshape((-1, keys.shape[1])) self.attention_weights = nn.functional.softmax( -((queries - keys) * self.w)**2 / 2, dim=1) # values的形状为(查询个数，“键－值”对个数) return torch.bmm(self.attention_weights.unsqueeze(1), values.unsqueeze(-1)).reshape(-1) # X_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输入 X_tile = x_train.repeat((n_train, 1)) # Y_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输出 Y_tile = y_train.repeat((n_train, 1)) # keys的形状:(\u0026#39;n_train\u0026#39;，\u0026#39;n_train\u0026#39;-1) keys = X_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1)) # values的形状:(\u0026#39;n_train\u0026#39;，\u0026#39;n_train\u0026#39;-1) values = Y_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1)) net = NWKernelRegression() loss = nn.MSELoss(reduction=\u0026#39;none\u0026#39;) trainer = torch.optim.SGD(net.parameters(), lr=0.5) animator = d2l.Animator(xlabel=\u0026#39;epoch\u0026#39;, ylabel=\u0026#39;loss\u0026#39;, xlim=[1, 5]) for epoch in range(5): trainer.zero_grad() l = loss(net(x_train, keys, values), y_train) l.sum().backward() trainer.step() print(f\u0026#39;epoch {epoch + 1}, loss {float(l.sum()):.6f}\u0026#39;) animator.add(epoch + 1, float(l.sum())) # keys的形状:(n_test，n_train)，每一行包含着相同的训练输入（例如，相同的键） keys = x_train.repeat((n_test, 1)) # value的形状:(n_test，n_train) values = y_train.repeat((n_test, 1)) y_hat = net(x_test, keys, values).unsqueeze(1).detach() plot_kernel_reg(y_hat) 1 2 3 d2l.show_heatmaps(net.attention_weights.unsqueeze(0).unsqueeze(0), xlabel=\u0026#39;Sorted training inputs\u0026#39;, ylabel=\u0026#39;Sorted testing inputs\u0026#39;) 注意力分数 主要思想 注意力分数是 query 和 key 的相似度，softmax 后得到注意力权重\n该式中，x_i 就是 key，x 就是 query，函数 a 为-1/2(x - x_i)^2，在经过 softmax 后变成注意力权重，与对应的值相乘后累加得到输出\n拓展到高维度 掩蔽 softmax 操作 1 2 3 4 import math import torch from torch import nn from d2l import torch as d2l 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def masked_softmax(X, valid_lens): \u0026#34;\u0026#34;\u0026#34;通过在最后一个轴上掩蔽元素来执行softmax操作\u0026#34;\u0026#34;\u0026#34; # X:3D张量，valid_lens:1D或2D张量 if valid_lens is None: return nn.functional.softmax(X, dim=-1) else: shape = X.shape if valid_lens.dim() == 1: # 对valid_lens扩展使其匹配每个query的有效长度 valid_lens = torch.repeat_interleave(valid_lens, shape[1]) else: # 展平成一维 valid_lens = valid_lens.reshape(-1) # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0 X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6) return nn.functional.softmax(X.reshape(shape), dim=-1) masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3])) # tensor([[[0.5980, 0.4020, 0.0000, 0.0000], # [0.5548, 0.4452, 0.0000, 0.0000]], # [[0.3716, 0.3926, 0.2358, 0.0000], # [0.3455, 0.3337, 0.3208, 0.0000]]]) masked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]])) # tensor([[[1.0000, 0.0000, 0.0000, 0.0000], # [0.4125, 0.3273, 0.2602, 0.0000]], # [[0.5254, 0.4746, 0.0000, 0.0000], # [0.3117, 0.2130, 0.1801, 0.2952]]]) 加性注意力 Additive Attention 将 key 和 query 乘上自己的可学参数后相加，放入一个隐藏大小为 h 输出大小为 1 的单隐藏层 MLP\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class AdditiveAttention(nn.Module): \u0026#34;\u0026#34;\u0026#34;加性注意力\u0026#34;\u0026#34;\u0026#34; def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs): super(AdditiveAttention, self).__init__(**kwargs) self.W_k = nn.Linear(key_size, num_hiddens, bias=False) self.W_q = nn.Linear(query_size, num_hiddens, bias=False) self.w_v = nn.Linear(num_hiddens, 1, bias=False) self.dropout = nn.Dropout(dropout) def forward(self, queries, keys, values, valid_lens): # queries的形状：(batch_size，查询的个数，query_size) # 通过Linear层将k和q转换成相同size queries, keys = self.W_q(queries), self.W_k(keys) # 在维度扩展后， # queries的形状：(batch_size，查询的个数，1，num_hidden) # key的形状：(batch_size，1，“键－值”对的个数，num_hiddens) # 使用广播方式进行求和 features = queries.unsqueeze(2) + keys.unsqueeze(1) features = torch.tanh(features) # self.w_v仅有一个输出，因此从形状中移除最后那个维度。 # scores的形状：(batch_size，查询的个数，“键-值”对的个数) scores = self.w_v(features).squeeze(-1) # 移除最后一个维度num_hiddens # masked_softmax中的掩蔽作用于scores中最后一个维度，也就是key中“键－值”对的个数 self.attention_weights = masked_softmax(scores, valid_lens) # values的形状：(batch_size，“键－值”对的个数，值的维度) return torch.bmm(self.dropout(self.attention_weights), values) # 输出形状：（批量大小，查询的步数，值的维度） queries, keys = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2)) # values的小批量，两个值矩阵是相同的 values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat( 2, 1, 1) valid_lens = torch.tensor([2, 6]) attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8, dropout=0.1) attention.eval() attention(queries, keys, values, valid_lens) # tensor([[[ 2.0000, 3.0000, 4.0000, 5.0000]], # [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=\u0026lt;BmmBackward0\u0026gt;) d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)), xlabel=\u0026#39;Keys\u0026#39;, ylabel=\u0026#39;Queries\u0026#39;) 缩放点积注意力 Scaled Dot Product Attention 几何角度解释：两向量方向越接近，点积越大\n当 q 和 k 的长度都为 d 时，做内积，然后除以$ \\sqrt{d} $来避免对长度敏感\n从小批量角度考虑：基于 n 个 query 和 m 个 k-v 其中 Q(n, d), K(m, d), V(m, v)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #@save class DotProductAttention(nn.Module): \u0026#34;\u0026#34;\u0026#34;缩放点积注意力\u0026#34;\u0026#34;\u0026#34; def __init__(self, dropout, **kwargs): super(DotProductAttention, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) # queries和keys的size均为d # queries的形状：(batch_size，查询的个数，d) # keys的形状：(batch_size，“键－值”对的个数，d) # values的形状：(batch_size，“键－值”对的个数，值的维度) # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数) def forward(self, queries, keys, values, valid_lens=None): d = queries.shape[-1] # 设置transpose_b=True为了交换keys的最后两个维度做转置 scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d) self.attention_weights = masked_softmax(scores, valid_lens) return torch.bmm(self.dropout(self.attention_weights), values) queries = torch.normal(0, 1, (2, 1, 2)) attention = DotProductAttention(dropout=0.5) attention.eval() attention(queries, keys, values, valid_lens) # tensor([[[ 2.0000, 3.0000, 4.0000, 5.0000]], # [[10.0000, 11.0000, 12.0000, 13.0000]]]) d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)), xlabel=\u0026#39;Keys\u0026#39;, ylabel=\u0026#39;Queries\u0026#39;) 注意力机制的 seq2seq 思路 机器翻译中，每个生成的词可能相关于源句子中不同的词\n在翻译时希望将注意力关注在源句子中对应的部分\n传统 seq2seq 会将 RNN 中最后一个 state 作为解码时的初始状态\n源句子中的每个词都会作为一个 key-value 放入 attention 中\nquery 则是去源句中对翻译的上一个词对应部分的附近进行查找，即把解码器的输出作为 query\nBahdanau 注意力 1 2 3 import torch from torch import nn from d2l import torch as d2l 注意力解码器\n编码器不变，因为 attention 只作用在 decoder 上\n1 2 3 4 5 6 7 8 class AttentionDecoder(d2l.Decoder): \u0026#34;\u0026#34;\u0026#34;带有注意力机制解码器的基本接口\u0026#34;\u0026#34;\u0026#34; def __init__(self, **kwargs): super(AttentionDecoder, self).__init__(**kwargs) @property # 在子类中让attention_weights可通过属性访问，使代码易读易维护 def attention_weights(self): raise NotImplementedError 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class Seq2SeqAttentionDecoder(AttentionDecoder): def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqAttentionDecoder, self).__init__(**kwargs) # key, value, query的长度均为num_hiddens # 添加加性注意力，可以学习参数，相对来说效果更好 self.attention = d2l.AdditiveAttention( num_hiddens, num_hiddens, num_hiddens, dropout) self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = nn.GRU( embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout) self.dense = nn.Linear(num_hiddens, vocab_size) def init_state(self, enc_outputs, enc_valid_lens, *args): # outputs的形状为(batch_size，num_steps，num_hiddens). # hidden_state的形状为(num_layers，batch_size，num_hiddens) outputs, hidden_state = enc_outputs # (batch_size, seq_lens, h) return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens) def forward(self, X, state): # enc_outputs的形状为(batch_size,num_steps,num_hiddens). # hidden_state的形状为(num_layers,batch_size, # num_hiddens) enc_outputs, hidden_state, enc_valid_lens = state # 输出X的形状为(num_steps,batch_size,embed_size) X = self.embedding(X).permute(1, 0, 2) outputs, self._attention_weights = [], [] for x in X: # query的形状为(batch_size,1,num_hiddens)，使上一个时刻最后一层的输出 query = torch.unsqueeze(hidden_state[-1], dim=1) # context的形状为(batch_size,1,num_hiddens) # 这里的key和value（即enc_outputs, enc_outputs）均为源句输出，只有query在变 context = self.attention( query, enc_outputs, enc_outputs, enc_valid_lens) # 在特征维度上连结 x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1) # 将x变形为(1,batch_size,embed_size+num_hiddens) out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state) outputs.append(out) self._attention_weights.append(self.attention.attention_weights) # 全连接层变换后，outputs的形状为 # (num_steps,batch_size,vocab_size) outputs = self.dense(torch.cat(outputs, dim=0)) return outputs.permute(1, 0, 2), [enc_outputs, hidden_state, enc_valid_lens] @property def attention_weights(self): return self._attention_weights 1 2 3 4 5 6 7 8 9 10 11 encoder = d2l.Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2) encoder.eval() decoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2) decoder.eval() X = torch.zeros((4, 7), dtype=torch.long) # (batch_size,num_steps) state = decoder.init_state(encoder(X), None) output, state = decoder(X, state) output.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape # (torch.Size([4, 7, 10]), 3, torch.Size([4, 7, 16]), 2, torch.Size([4, 16])) 训练\n1 2 3 4 5 6 7 8 9 10 11 embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1 batch_size, num_steps = 64, 10 lr, num_epochs, device = 0.005, 250, d2l.try_gpu() train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps) encoder = d2l.Seq2SeqEncoder( len(src_vocab), embed_size, num_hiddens, num_layers, dropout) decoder = Seq2SeqAttentionDecoder( len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout) net = d2l.EncoderDecoder(encoder, decoder) d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device) 1 2 3 4 5 6 7 8 9 10 11 engs = [\u0026#39;go .\u0026#39;, \u0026#34;i lost .\u0026#34;, \u0026#39;he\\\u0026#39;s calm .\u0026#39;, \u0026#39;i\\\u0026#39;m home .\u0026#39;] fras = [\u0026#39;va !\u0026#39;, \u0026#39;j\\\u0026#39;ai perdu .\u0026#39;, \u0026#39;il est calme .\u0026#39;, \u0026#39;je suis chez moi .\u0026#39;] for eng, fra in zip(engs, fras): translation, dec_attention_weight_seq = d2l.predict_seq2seq( net, eng, src_vocab, tgt_vocab, num_steps, device, True) print(f\u0026#39;{eng} =\u0026gt; {translation}, \u0026#39;, f\u0026#39;bleu {d2l.bleu(translation, fra, k=2):.3f}\u0026#39;) # go . =\u0026gt; va !, bleu 1.000 # i lost . =\u0026gt; j\u0026#39;ai perdu ., bleu 1.000 # he\u0026#39;s calm . =\u0026gt; il est paresseux ., bleu 0.658 # i\u0026#39;m home . =\u0026gt; je suis chez moi ., bleu 1.000 1 2 3 4 5 6 attention_weights = torch.cat([step[0][0][0] for step in dec_attention_weight_seq], 0).reshape(( 1, 1, -1, num_steps)) # 加上一个包含序列结束词元 d2l.show_heatmaps( attention_weights[:, :, :, :len(engs[-1].split()) + 1].cpu(), xlabel=\u0026#39;Key positions\u0026#39;, ylabel=\u0026#39;Query positions\u0026#39;) 自注意力和位置编码 自注意力 将 x_i 作为 query，所有 (x, x) 作为 key-value 对序列抽取特征得到 y_i\n比较 CNN、RNN、Self-attention k: kernel 大小\nn: 序列长度\nd: 输入输出通道数\n自注意力机制适合用于长文本，但这也使得计算量非常大\n位置编码 于 CNN/RNN 不同，自注意力没有记录位置信息 位置编码将位置信息注入到输入中 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #@save class PositionalEncoding(nn.Module): \u0026#34;\u0026#34;\u0026#34;位置编码\u0026#34;\u0026#34;\u0026#34; def __init__(self, num_hiddens, dropout, max_len=1000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(dropout) # 创建一个足够长的P self.P = torch.zeros((1, max_len, num_hiddens)) X = torch.arange(max_len, dtype=torch.float32).reshape( -1, 1) / torch.pow(10000, torch.arange( 0, num_hiddens, 2, dtype=torch.float32) / num_hiddens) self.P[:, :, 0::2] = torch.sin(X) self.P[:, :, 1::2] = torch.cos(X) def forward(self, X): X = X + self.P[:, :X.shape[1], :].to(X.device) return self.dropout(X) 绝对位置信息\n计算机使用二进制编码，越低位次变化的频率越大（例如最低位总是 01 交替）\n相对位置信息\n可看出不管 (p_i, p_{i+d}) 在任何位置，其$ \\omega_j $的值都相同\n$ \\omega $因此可以学到不同词之间的相对位置\nTransformer 相比于 seq2seq，Transformer 没有使用 RNN，是纯基于注意力 多头注意力 思想 输入的 q、k、v： 头 i 的可学习参数（全连接层）： 头 i 的输出（注意力层）： 输出的可学习参数（顶部全连接层）： 多头注意力的输出： 有掩码的多头注意力 attention 中没有时间先后的概念。这在编码器中适用，但在解码器中，对一个元素输出时不应该考虑该元素之后的元素 通过掩码实现：计算 x_i 的输出时，假设当前序列长度为 i，计算 softmax 时后续序列无权重 代码实现 1 2 3 4 import math import torch from torch import nn from d2l import torch as d2l 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 #@save class MultiHeadAttention(nn.Module): \u0026#34;\u0026#34;\u0026#34;多头注意力\u0026#34;\u0026#34;\u0026#34; def __init__(self, key_size, query_size, value_size, num_hiddens, num_heads, dropout, bias=False, **kwargs): super(MultiHeadAttention, self).__init__(**kwargs) self.num_heads = num_heads self.attention = d2l.DotProductAttention(dropout) self.W_q = nn.Linear(query_size, num_hiddens, bias=bias) self.W_k = nn.Linear(key_size, num_hiddens, bias=bias) self.W_v = nn.Linear(value_size, num_hiddens, bias=bias) self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias) def forward(self, queries, keys, values, valid_lens): # queries，keys，values的形状: # (batch_size，查询或者“键－值”对的个数，num_hiddens) # valid_lens　的形状: # (batch_size，)或(batch_size，查询的个数) # 经过变换后，输出的queries，keys，values　的形状: # (batch_size*num_heads，查询或者“键－值”对的个数， # num_hiddens/num_heads) # transpose_qkv的变换使得每个头可以并行计算 queries = transpose_qkv(self.W_q(queries), self.num_heads) keys = transpose_qkv(self.W_k(keys), self.num_heads) values = transpose_qkv(self.W_v(values), self.num_heads) # 把多个头合成一个大头，使得可以并行计算 if valid_lens is not None: # 在轴0，将第一项（标量或者矢量）复制num_heads次， # 然后如此复制第二项，然后诸如此类。 valid_lens = torch.repeat_interleave( valid_lens, repeats=self.num_heads, dim=0) # output的形状:(batch_size*num_heads，查询的个数， # num_hiddens/num_heads) output = self.attention(queries, keys, values, valid_lens) # output_concat的形状:(batch_size，查询的个数，num_hiddens) output_concat = transpose_output(output, self.num_heads) return self.W_o(output_concat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #@save def transpose_qkv(X, num_heads): \u0026#34;\u0026#34;\u0026#34;为了多注意力头的并行计算而变换形状\u0026#34;\u0026#34;\u0026#34; # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens) # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads， # num_hiddens/num_heads) X = X.reshape(X.shape[0], X.shape[1], num_heads, -1) # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数, # num_hiddens/num_heads) X = X.permute(0, 2, 1, 3) # 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数, # num_hiddens/num_heads) return X.reshape(-1, X.shape[2], X.shape[3]) #@save def transpose_output(X, num_heads): \u0026#34;\u0026#34;\u0026#34;逆转transpose_qkv函数的操作\u0026#34;\u0026#34;\u0026#34; X = X.reshape(-1, num_heads, X.shape[1], X.shape[2]) X = X.permute(0, 2, 1, 3) return X.reshape(X.shape[0], X.shape[1], -1) 基于位置的前馈网络 两个全连接层，等价于两层核窗口为 1 的一维卷积层 形状由(bn, d)变换成(bn, d)，再变化回(b, n, d) 1 2 3 4 5 import math import pandas as pd import torch from torch import nn from d2l import torch as d2l 1 2 3 4 5 6 7 8 9 10 11 12 # 输入不是二维时，会将最后一个维度视为feature，之前的维度全部视作样本维 class PositionWiseFFN(nn.Module): \u0026#34;\u0026#34;\u0026#34;基于位置的前馈网络\u0026#34;\u0026#34;\u0026#34; def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs): super(PositionWiseFFN, self).__init__(**kwargs) self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens) self.relu = nn.ReLU() self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs) def forward(self, X): return self.dense2(self.relu(self.dense1(X))) 1 2 3 4 5 6 7 8 9 ffn = PositionWiseFFN(4, 4, 8) ffn.eval() ffn(torch.ones((2, 3, 4)))[0] # tensor([[-0.8290, 1.0067, 0.3619, 0.3594, -0.5328, 0.2712, 0.7394, 0.0747], # [-0.8290, 1.0067, 0.3619, 0.3594, -0.5328, 0.2712, 0.7394, 0.0747], # [-0.8290, 1.0067, 0.3619, 0.3594, -0.5328, 0.2712, 0.7394, 0.0747]], # grad_fn=\u0026lt;SelectBackward0\u0026gt;) # 改变张量的最里层维度的尺寸，会改变成基于位置的前馈网络的输出尺寸 残差连接和层归一化 图中 Block 可以是多头注意力、前馈网络等，残差连接后进行归一化\n在归一化时要进行层归一化而不是批量归一化，层归一化操作的数据归属于一个序列 图中沿 b 维度是一个个 batch，沿 d 是序列的特征向量，因此垂直于 b 切开刚好是一个个序列\n1 2 3 4 5 6 7 8 9 class AddNorm(nn.Module): \u0026#34;\u0026#34;\u0026#34;残差连接后进行层规范化\u0026#34;\u0026#34;\u0026#34; def __init__(self, normalized_shape, dropout, **kwargs): super(AddNorm, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) self.ln = nn.LayerNorm(normalized_shape) def forward(self, X, Y): return self.ln(self.dropout(Y) + X) 编码器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #@save class EncoderBlock(nn.Module): \u0026#34;\u0026#34;\u0026#34;Transformer编码器块\u0026#34;\u0026#34;\u0026#34; def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias=False, **kwargs): super(EncoderBlock, self).__init__(**kwargs) self.attention = d2l.MultiHeadAttention( key_size, query_size, value_size, num_hiddens, num_heads, dropout, use_bias) self.addnorm1 = AddNorm(norm_shape, dropout) self.ffn = PositionWiseFFN( ffn_num_input, ffn_num_hiddens, num_hiddens) self.addnorm2 = AddNorm(norm_shape, dropout) def forward(self, X, valid_lens): Y = self.addnorm1(X, self.attention(X, X, X, valid_lens)) return self.addnorm2(Y, self.ffn(Y)) 注：Transformer编码器中的任何层都不会改变其输入的形状\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class TransformerEncoder(d2l.Encoder): \u0026#34;\u0026#34;\u0026#34;Transformer编码器\u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, use_bias=False, **kwargs): super(TransformerEncoder, self).__init__(**kwargs) self.num_hiddens = num_hiddens self.embedding = nn.Embedding(vocab_size, num_hiddens) self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout) self.blks = nn.Sequential() for i in range(num_layers): self.blks.add_module(\u0026#34;block\u0026#34;+str(i), EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias)) def forward(self, X, valid_lens, *args): # 因为位置编码值在-1和1之间， # 因此嵌入值乘以嵌入维度的平方根进行缩放， # 然后再与位置编码相加。 X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens)) self.attention_weights = [None] * len(self.blks) for i, blk in enumerate(self.blks): X = blk(X, valid_lens) self.attention_weights[ i] = blk.attention.attention.attention_weights return X 解码器 编码器输出的向量 y_1, \u0026hellip; , y_n 作为解码器中第 i 个 Transformer 块中多头注意力的 key 和 value，query 取自目标序列\n这使得编码器和解码器中的块的个数和输出维度相同\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class DecoderBlock(nn.Module): \u0026#34;\u0026#34;\u0026#34;解码器中第i个块\u0026#34;\u0026#34;\u0026#34; def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, **kwargs): super(DecoderBlock, self).__init__(**kwargs) self.i = i self.attention1 = d2l.MultiHeadAttention( key_size, query_size, value_size, num_hiddens, num_heads, dropout) self.addnorm1 = AddNorm(norm_shape, dropout) self.attention2 = d2l.MultiHeadAttention( key_size, query_size, value_size, num_hiddens, num_heads, dropout) self.addnorm2 = AddNorm(norm_shape, dropout) self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens) self.addnorm3 = AddNorm(norm_shape, dropout) def forward(self, X, state): enc_outputs, enc_valid_lens = state[0], state[1] # 训练阶段，输出序列的所有词元都在同一时间处理， # 因此state[2][self.i]初始化为None。 # 预测阶段，输出序列是通过词元一个接着一个解码的， # 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示 if state[2][self.i] is None: key_values = X else: key_values = torch.cat((state[2][self.i], X), axis=1) state[2][self.i] = key_values if self.training: batch_size, num_steps, _ = X.shape # dec_valid_lens的开头:(batch_size,num_steps), # 其中每一行是[1,2,...,num_steps] dec_valid_lens = torch.arange( 1, num_steps + 1, device=X.device).repeat(batch_size, 1) else: dec_valid_lens = None # 自注意力 X2 = self.attention1(X, key_values, key_values, dec_valid_lens) Y = self.addnorm1(X, X2) # 编码器－解码器注意力。 # enc_outputs的开头:(batch_size,num_steps,num_hiddens) Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens) Z = self.addnorm2(Y, Y2) return self.addnorm3(Z, self.ffn(Z)), state 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class TransformerDecoder(d2l.AttentionDecoder): def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, **kwargs): super(TransformerDecoder, self).__init__(**kwargs) self.num_hiddens = num_hiddens self.num_layers = num_layers self.embedding = nn.Embedding(vocab_size, num_hiddens) self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout) self.blks = nn.Sequential() for i in range(num_layers): self.blks.add_module(\u0026#34;block\u0026#34;+str(i), DecoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i)) self.dense = nn.Linear(num_hiddens, vocab_size) def init_state(self, enc_outputs, enc_valid_lens, *args): return [enc_outputs, enc_valid_lens, [None] * self.num_layers] def forward(self, X, state): X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens)) self._attention_weights = [[None] * len(self.blks) for _ in range (2)] for i, blk in enumerate(self.blks): X, state = blk(X, state) # 解码器自注意力权重 self._attention_weights[0][ i] = blk.attention1.attention.attention_weights # “编码器－解码器”自注意力权重 self._attention_weights[1][ i] = blk.attention2.attention.attention_weights return self.dense(X), state @property def attention_weights(self): return self._attention_weights 训练 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 64, 10 lr, num_epochs, device = 0.005, 200, d2l.try_gpu() ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4 key_size, query_size, value_size = 32, 32, 32 norm_shape = [32] train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps) encoder = TransformerEncoder( len(src_vocab), key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout) decoder = TransformerDecoder( len(tgt_vocab), key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout) net = d2l.EncoderDecoder(encoder, decoder) d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device) 预测 预测第 t+1 个输出时，前 t 个预测值作为 key 和 value，第 t 个预测值作为 query\n","date":"2024-09-27T00:00:00Z","permalink":"https://changle-liu.github.io/p/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","title":"注意力机制-动手学深度学习"},{"content":"动手学深度学习v2\n课程链接：https://courses.d2l.ai/zh-v2/\n门控循环单元 GRU 主要思想 不是每一个观察值都同等重要 更新门：关注 重置门：遗忘 候选隐状态\n$ R_t $中的值保持在 [0, 1] 范围内，代表了对过去状态的记忆程度，接近于 0 时会将先前的状态大幅度遗忘\n隐状态\n$ Z_t $趋于 1 时，等于直接照搬过去的状态，不考虑这一轮的$ X_t $\n$ Z_t $趋于 0 时，只考虑目前的状态，等价回 RNN\n$ R_t $控制保留过去多少状态，$ Z_t $控制保留这一轮多少状态\n代码实现 从零实现 1 2 3 4 5 6 import torch from torch import nn from d2l import torch as d2l batch_size, num_steps = 32, 35 train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps) 初始化参数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def get_params(vocab_size, num_hiddens, device): num_inputs = num_outputs = vocab_size def normal(shape): return torch.randn(size=shape, device=device)*0.01 def three(): return (normal((num_inputs, num_hiddens)), normal((num_hiddens, num_hiddens)), torch.zeros(num_hiddens, device=device)) # 初始化 W_xz, W_hz, b_z = three() # 更新门参数 W_xr, W_hr, b_r = three() # 重置门参数 W_xh, W_hh, b_h = three() # 候选隐状态参数 # 输出层参数 W_hq = normal((num_hiddens, num_outputs)) b_q = torch.zeros(num_outputs, device=device) # 附加梯度 params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q] for param in params: param.requires_grad_(True) return params 定义模型\n1 2 def init_gru_state(batch_size, num_hiddens, device): return (torch.zeros((batch_size, num_hiddens), device=device), ) 1 2 3 4 5 6 7 8 9 10 11 12 def gru(inputs, state, params): W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params H, = state outputs = [] for X in inputs: Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z) R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r) H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h) H = Z * H + (1 - Z) * H_tilda Y = H @ W_hq + b_q outputs.append(Y) return torch.cat(outputs, dim=0), (H,) 训练与预测\n1 2 3 4 5 vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu() num_epochs, lr = 500, 1 model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_params, init_gru_state, gru) d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device) 简洁实现 1 2 3 4 5 num_inputs = vocab_size gru_layer = nn.GRU(num_inputs, num_hiddens) model = d2l.RNNModel(gru_layer, len(vocab)) model = model.to(device) d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device) 长短期记忆网络 LSTM 主要思想 忘记门 输入门 输出门 候选记忆元\n记忆元\n$ F_t $: 是否遗忘该轮$ X_t $\n$ I_t $: 是否记忆该轮$ X_t $\n和 GRU 不同的是，LSTM 中的 F_t 和 I_t 是独立的，可以既保留过去记忆又保留当前记忆\n隐状态\n将 C_t 的值限制在 (-1, 1)中，并使用 O_t 控制输出\n代码实现 1 2 3 4 5 6 import torch from torch import nn from d2l import torch as d2l batch_size, num_steps = 32, 35 train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps) 初始化参数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def get_lstm_params(vocab_size, num_hiddens, device): num_inputs = num_outputs = vocab_size def normal(shape): return torch.randn(size=shape, device=device)*0.01 def three(): return (normal((num_inputs, num_hiddens)), normal((num_hiddens, num_hiddens)), torch.zeros(num_hiddens, device=device)) W_xi, W_hi, b_i = three() # 输入门参数 W_xf, W_hf, b_f = three() # 遗忘门参数 W_xo, W_ho, b_o = three() # 输出门参数 W_xc, W_hc, b_c = three() # 候选记忆元参数 # 输出层参数 W_hq = normal((num_hiddens, num_outputs)) b_q = torch.zeros(num_outputs, device=device) # 附加梯度 params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q] for param in params: param.requires_grad_(True) return params 定义模型\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 初始化H和C def init_lstm_state(batch_size, num_hiddens, device): return (torch.zeros((batch_size, num_hiddens), device=device), torch.zeros((batch_size, num_hiddens), device=device)) def lstm(inputs, state, params): [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q] = params (H, C) = state outputs = [] for X in inputs: I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i) F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f) O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o) C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c) C = F * C + I * C_tilda H = O * torch.tanh(C) Y = (H @ W_hq) + b_q outputs.append(Y) return torch.cat(outputs, dim=0), (H, C) 训练和预测\n1 2 3 4 5 vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu() num_epochs, lr = 500, 1 model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_lstm_params, init_lstm_state, lstm) d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device) 简洁实现\n1 2 3 4 5 num_inputs = vocab_size lstm_layer = nn.LSTM(num_inputs, num_hiddens) model = d2l.RNNModel(lstm_layer, len(vocab)) model = model.to(device) d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device) 深度循环神经网络 1 2 3 4 5 6 import torch from torch import nn from d2l import torch as d2l batch_size, num_steps = 32, 35 train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps) 1 2 3 4 5 6 7 8 9 vocab_size, num_hiddens, num_layers = len(vocab), 256, 2 num_inputs = vocab_size device = d2l.try_gpu() lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers) model = d2l.RNNModel(lstm_layer, len(vocab)) model = model.to(device) num_epochs, lr = 500, 2 d2l.train_ch8(model, train_iter, vocab, lr*1.0, num_epochs, device) 双向循环神经网络 双向 RNN 主要用来对句子进行特征提取或填空，通常不用来推理\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import torch from torch import nn from d2l import torch as d2l # 加载数据 batch_size, num_steps, device = 32, 35, d2l.try_gpu() train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps) # 通过设置“bidirective=True”来定义双向LSTM模型 vocab_size, num_hiddens, num_layers = len(vocab), 256, 2 num_inputs = vocab_size lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=True) model = d2l.RNNModel(lstm_layer, len(vocab)) model = model.to(device) # 训练模型 num_epochs, lr = 500, 1 d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device) 机器翻译与数据集 1 2 3 import os import torch from d2l import torch as d2l 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 d2l.DATA_HUB[\u0026#39;fra-eng\u0026#39;] = (d2l.DATA_URL + \u0026#39;fra-eng.zip\u0026#39;, \u0026#39;94646ad1522d915e7b0f9296181140edcf86a4f5\u0026#39;) def read_data_nmt(): \u0026#34;\u0026#34;\u0026#34;载入“英语－法语”数据集\u0026#34;\u0026#34;\u0026#34; data_dir = d2l.download_extract(\u0026#39;fra-eng\u0026#39;) with open(os.path.join(data_dir, \u0026#39;fra.txt\u0026#39;), \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: return f.read() raw_text = read_data_nmt() print(raw_text[:75]) # Go. Va ! # Hi. Salut ! # Run! Cours ! # Run! Courez ! # Who? Qui ? # Wow! Ça alors ! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def preprocess_nmt(text): \u0026#34;\u0026#34;\u0026#34;预处理“英语－法语”数据集\u0026#34;\u0026#34;\u0026#34; # 判断字符是否是标点且标点前无空格 def no_space(char, prev_char): return char in set(\u0026#39;,.!?\u0026#39;) and prev_char != \u0026#39; \u0026#39; # 使用空格替换不间断空格 # 使用小写字母替换大写字母 text = text.replace(\u0026#39;\\u202f\u0026#39;, \u0026#39; \u0026#39;).replace(\u0026#39;\\xa0\u0026#39;, \u0026#39; \u0026#39;).lower() # 在单词和标点符号之间插入空格 out = [\u0026#39; \u0026#39; + char if i \u0026gt; 0 and no_space(char, text[i - 1]) else char for i, char in enumerate(text)] return \u0026#39;\u0026#39;.join(out) text = preprocess_nmt(raw_text) print(text[:80]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def tokenize_nmt(text, num_examples=None): \u0026#34;\u0026#34;\u0026#34;词元化“英语－法语”数据数据集\u0026#34;\u0026#34;\u0026#34; source, target = [], [] for i, line in enumerate(text.split(\u0026#39;\\n\u0026#39;)): if num_examples and i \u0026gt; num_examples: break parts = line.split(\u0026#39;\\t\u0026#39;) if len(parts) == 2: source.append(parts[0].split(\u0026#39; \u0026#39;)) target.append(parts[1].split(\u0026#39; \u0026#39;)) return source, target source, target = tokenize_nmt(text) source[:6], target[:6] 1 2 3 4 5 6 7 8 9 10 11 12 13 def show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist): \u0026#34;\u0026#34;\u0026#34;绘制列表长度对的直方图\u0026#34;\u0026#34;\u0026#34; d2l.set_figsize() _, _, patches = d2l.plt.hist( [[len(l) for l in xlist], [len(l) for l in ylist]]) d2l.plt.xlabel(xlabel) d2l.plt.ylabel(ylabel) for patch in patches[1].patches: patch.set_hatch(\u0026#39;/\u0026#39;) d2l.plt.legend(legend) show_list_len_pair_hist([\u0026#39;source\u0026#39;, \u0026#39;target\u0026#39;], \u0026#39;# tokens per sequence\u0026#39;, \u0026#39;count\u0026#39;, source, target); 1 2 3 4 5 6 # 出现次数少于2次的视为低频率词元 # 指定填充词元、开始词元和结束词元 src_vocab = d2l.Vocab(source, min_freq=2, reserved_tokens=[\u0026#39;\u0026lt;pad\u0026gt;\u0026#39;, \u0026#39;\u0026lt;bos\u0026gt;\u0026#39;, \u0026#39;\u0026lt;eos\u0026gt;\u0026#39;]) len(src_vocab) # 10012 1 2 3 4 5 6 7 8 9 def truncate_pad(line, num_steps, padding_token): \u0026#34;\u0026#34;\u0026#34;截断或填充文本序列\u0026#34;\u0026#34;\u0026#34; # 截断长度大于num_steps的文本 if len(line) \u0026gt; num_steps: return line[:num_steps] # 截断 return line + [padding_token] * (num_steps - len(line)) # 填充 truncate_pad(src_vocab[source[0]], 10, src_vocab[\u0026#39;\u0026lt;pad\u0026gt;\u0026#39;]) # [47, 4, 1, 1, 1, 1, 1, 1, 1, 1] 1 2 3 4 5 6 7 8 9 10 11 def build_array_nmt(lines, vocab, num_steps): \u0026#34;\u0026#34;\u0026#34;将机器翻译的文本序列转换成小批量\u0026#34;\u0026#34;\u0026#34; # 映射到词表 lines = [vocab[l] for l in lines] # 句尾加上结束词元 lines = [l + [vocab[\u0026#39;\u0026lt;eos\u0026gt;\u0026#39;]] for l in lines] array = torch.tensor([truncate_pad( l, num_steps, vocab[\u0026#39;\u0026lt;pad\u0026gt;\u0026#39;]) for l in lines]) valid_len = (array != vocab[\u0026#39;\u0026lt;pad\u0026gt;\u0026#39;]).type(torch.int32).sum(1) return array, valid_len # valid_len: 文本的实际长度 训练模型\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #@save def load_data_nmt(batch_size, num_steps, num_examples=600): \u0026#34;\u0026#34;\u0026#34;返回翻译数据集的迭代器和词表\u0026#34;\u0026#34;\u0026#34; text = preprocess_nmt(read_data_nmt()) source, target = tokenize_nmt(text, num_examples) src_vocab = d2l.Vocab(source, min_freq=2, reserved_tokens=[\u0026#39;\u0026lt;pad\u0026gt;\u0026#39;, \u0026#39;\u0026lt;bos\u0026gt;\u0026#39;, \u0026#39;\u0026lt;eos\u0026gt;\u0026#39;]) tgt_vocab = d2l.Vocab(target, min_freq=2, reserved_tokens=[\u0026#39;\u0026lt;pad\u0026gt;\u0026#39;, \u0026#39;\u0026lt;bos\u0026gt;\u0026#39;, \u0026#39;\u0026lt;eos\u0026gt;\u0026#39;]) src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps) tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps) data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len) data_iter = d2l.load_array(data_arrays, batch_size) return data_iter, src_vocab, tgt_vocab # 迭代器返回原句，原句合法长度，target，target合法长度 1 2 3 4 5 6 7 8 9 10 11 12 13 train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=2, num_steps=8) for X, X_valid_len, Y, Y_valid_len in train_iter: print(\u0026#39;X:\u0026#39;, X.type(torch.int32)) print(\u0026#39;X的有效长度:\u0026#39;, X_valid_len) print(\u0026#39;Y:\u0026#39;, Y.type(torch.int32)) print(\u0026#39;Y的有效长度:\u0026#39;, Y_valid_len) break # X: tensor([[ 7, 43, 4, 3, 1, 1, 1, 1], # [44, 23, 4, 3, 1, 1, 1, 1]], dtype=torch.int32) # X的有效长度: tensor([4, 4]) # Y: tensor([[ 6, 7, 40, 4, 3, 1, 1, 1], # [ 0, 5, 3, 1, 1, 1, 1, 1]], dtype=torch.int32) # Y的有效长度: tensor([5, 3]) 编码器和解码器 1 2 3 4 5 6 7 8 9 10 11 from torch import nn #@save class Encoder(nn.Module): \u0026#34;\u0026#34;\u0026#34;编码器-解码器架构的基本编码器接口\u0026#34;\u0026#34;\u0026#34; def __init__(self, **kwargs): super(Encoder, self).__init__(**kwargs) def forward(self, X, *args): raise NotImplementedError 1 2 3 4 5 6 7 8 9 10 11 12 13 #@save class Decoder(nn.Module): \u0026#34;\u0026#34;\u0026#34;编码器-解码器架构的基本解码器接口\u0026#34;\u0026#34;\u0026#34; def __init__(self, **kwargs): super(Decoder, self).__init__(**kwargs) # enc_outputs: 接收encoder的输出 def init_state(self, enc_outputs, *args): raise NotImplementedError # 接收encoder的state，同时允许额外的输入X def forward(self, X, state): raise NotImplementedError 1 2 3 4 5 6 7 8 9 10 11 12 #@save class EncoderDecoder(nn.Module): \u0026#34;\u0026#34;\u0026#34;编码器-解码器架构的基类\u0026#34;\u0026#34;\u0026#34; def __init__(self, encoder, decoder, **kwargs): super(EncoderDecoder, self).__init__(**kwargs) self.encoder = encoder self.decoder = decoder def forward(self, enc_X, dec_X, *args): enc_outputs = self.encoder(enc_X, *args) dec_state = self.decoder.init_state(enc_outputs, *args) return self.decoder(dec_X, dec_state) 序列到序列学习 seq2seq 主要思想 编码器是一个 RNN，读取输入句子 可以是双向 RNN 解码器是另一个 RNN，接收 encoder 的 state 来输出 编码器不需要输出，其最后时间步的隐状态作为解码器的初始隐状态 训练：\n预测：\n训练时 decoder 会使用目标句子作为输入，而预测时则只能使用上一个预测出的词作为输入 预测序列的评估 BLEU（bilingual evaluation understudy）\n$ p_n $是预测所有 n-gram 的精度， 是预测序列与标签序列中匹配的n元语法的数量， 和预测序列中n元语法的数量的比率\nBLEU 定义：\nexp 项用来惩罚过短的预测，求和项中的指数使得长匹配具有高权重\n代码实现 1 2 3 4 5 import collections import math import torch from torch import nn from d2l import torch as d2l 编码器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Seq2SeqEncoder(d2l.Encoder): \u0026#34;\u0026#34;\u0026#34;用于序列到序列学习的循环神经网络编码器\u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqEncoder, self).__init__(**kwargs) # 嵌入层：获得输入序列中每个词元的特征向量 w.shape = (词表大小, 特征向量维度) self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout) def forward(self, X, *args): # 输出\u0026#39;X\u0026#39;的形状：(batch_size,num_steps,embed_size) X = self.embedding(X) # 调换顺序：在循环神经网络模型中，第一个轴对应于时间步 X = X.permute(1, 0, 2) # 如果未提及状态，则默认为0 output, state = self.rnn(X) # output的形状:(num_steps,batch_size,num_hiddens) # state的形状:(num_layers,batch_size,num_hiddens) return output, state 1 2 3 4 5 6 7 8 9 10 encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2) encoder.eval() # 4: batch_size, 7: num_steps X = torch.zeros((4, 7), dtype=torch.long) output, state = encoder(X) output.shape # torch.Size([7, 4, 16]) state.shape # torch.Size([2, 4, 16]) 解码器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class Seq2SeqDecoder(d2l.Decoder): \u0026#34;\u0026#34;\u0026#34;用于序列到序列学习的循环神经网络解码器\u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqDecoder, self).__init__(**kwargs) # decoder的嵌入层和encoder不同，因为vocab不同 self.embedding = nn.Embedding(vocab_size, embed_size) # 输入层+nun_hiddens是因为decoder的输入加上了context self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout) # 不要忘了输出层 self.dense = nn.Linear(num_hiddens, vocab_size) def init_state(self, enc_outputs, *args): # enc_outputs = (output, state) return enc_outputs[1] def forward(self, X, state): # 输出\u0026#39;X\u0026#39;的形状：(batch_size,num_steps,embed_size) X = self.embedding(X).permute(1, 0, 2) # X的形状：(num_steps,batch_size,embed_size) # 广播context，将state[-1]重复num_steps次，使其具有与X相同的num_steps # state[-1]形状：(batch_size, num_hiddens) # context形状：(num_steps, batch_size, num_hiddens) context = state[-1].repeat(X.shape[0], 1, 1) X_and_context = torch.cat((X, context), 2) output, state = self.rnn(X_and_context, state) output = self.dense(output).permute(1, 0, 2) # output的形状:(batch_size,num_steps,vocab_size) # state的形状:(num_layers,batch_size,num_hiddens) return output, state # 实例化解码器 decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2) decoder.eval() state = decoder.init_state(encoder(X)) output, state = decoder(X, state) output.shape, state.shape # (torch.Size([4, 7, 10]), torch.Size([2, 4, 16])) 损失函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 def sequence_mask(X, valid_len, value=0): \u0026#34;\u0026#34;\u0026#34;在序列中屏蔽不相关的项\u0026#34;\u0026#34;\u0026#34; # 将超出valid_len长度的项标成value maxlen = X.size(1) mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] \u0026lt; valid_len[:, None] X[~mask] = value return X X = torch.tensor([[1, 2, 3], [4, 5, 6]]) sequence_mask(X, torch.tensor([1, 2])) # tensor([[1, 0, 0], # [4, 5, 0]]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class MaskedSoftmaxCELoss(nn.CrossEntropyLoss): \u0026#34;\u0026#34;\u0026#34;带遮蔽的softmax交叉熵损失函数\u0026#34;\u0026#34;\u0026#34; # pred的形状：(batch_size,num_steps,vocab_size) # label的形状：(batch_size,num_steps) # valid_len的形状：(batch_size,) def forward(self, pred, label, valid_len): weights = torch.ones_like(label) # 将w中超出的不相干部分变成0 weights = sequence_mask(weights, valid_len) # 关闭默认的平均或求和的损失计算模式，保持每个样本的损失值独立 self.reduction=\u0026#39;none\u0026#39; # 将 pred 的维度从 (batch_size, num_steps, vocab_size) # 变为 (batch_size, vocab_size, num_steps) unweighted_loss = super(MaskedSoftmaxCELoss, self).forward( pred.permute(0, 2, 1), label) weighted_loss = (unweighted_loss * weights).mean(dim=1) return weighted_loss loss = MaskedSoftmaxCELoss() loss(torch.ones(3, 4, 10), torch.ones((3, 4), dtype=torch.long), torch.tensor([4, 2, 0])) # tensor([2.3026, 1.1513, 0.0000]) 训练\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 #@save def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device): \u0026#34;\u0026#34;\u0026#34;训练序列到序列模型\u0026#34;\u0026#34;\u0026#34; def xavier_init_weights(m): if type(m) == nn.Linear: nn.init.xavier_uniform_(m.weight) if type(m) == nn.GRU: for param in m._flat_weights_names: if \u0026#34;weight\u0026#34; in param: nn.init.xavier_uniform_(m._parameters[param]) net.apply(xavier_init_weights) net.to(device) optimizer = torch.optim.Adam(net.parameters(), lr=lr) loss = MaskedSoftmaxCELoss() net.train() animator = d2l.Animator(xlabel=\u0026#39;epoch\u0026#39;, ylabel=\u0026#39;loss\u0026#39;, xlim=[10, num_epochs]) for epoch in range(num_epochs): timer = d2l.Timer() metric = d2l.Accumulator(2) # 训练损失总和，词元数量 for batch in data_iter: optimizer.zero_grad() X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch] bos = torch.tensor([tgt_vocab[\u0026#39;\u0026lt;bos\u0026gt;\u0026#39;]] * Y.shape[0], device=device).reshape(-1, 1) dec_input = torch.cat([bos, Y[:, :-1]], 1) # 强制教学 Y_hat, _ = net(X, dec_input, X_valid_len) l = loss(Y_hat, Y, Y_valid_len) l.sum().backward() # 损失函数的标量进行“反向传播” d2l.grad_clipping(net, 1) num_tokens = Y_valid_len.sum() optimizer.step() with torch.no_grad(): metric.add(l.sum(), num_tokens) if (epoch + 1) % 10 == 0: animator.add(epoch + 1, (metric[0] / metric[1],)) print(f\u0026#39;loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} \u0026#39; f\u0026#39;tokens/sec on {str(device)}\u0026#39;) 1 2 3 4 5 6 7 8 9 10 11 embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1 batch_size, num_steps = 64, 10 lr, num_epochs, device = 0.005, 300, d2l.try_gpu() train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps) encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout) decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout) net = d2l.EncoderDecoder(encoder, decoder) train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device) 预测\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 #@save def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps, device, save_attention_weights=False): \u0026#34;\u0026#34;\u0026#34;序列到序列模型的预测\u0026#34;\u0026#34;\u0026#34; # 在预测时将net设置为评估模式 net.eval() src_tokens = src_vocab[src_sentence.lower().split(\u0026#39; \u0026#39;)] + [ src_vocab[\u0026#39;\u0026lt;eos\u0026gt;\u0026#39;]] enc_valid_len = torch.tensor([len(src_tokens)], device=device) src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab[\u0026#39;\u0026lt;pad\u0026gt;\u0026#39;]) # 添加批量轴 enc_X = torch.unsqueeze( torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0) enc_outputs = net.encoder(enc_X, enc_valid_len) dec_state = net.decoder.init_state(enc_outputs, enc_valid_len) # 添加批量轴 dec_X = torch.unsqueeze(torch.tensor( [tgt_vocab[\u0026#39;\u0026lt;bos\u0026gt;\u0026#39;]], dtype=torch.long, device=device), dim=0) output_seq, attention_weight_seq = [], [] for _ in range(num_steps): Y, dec_state = net.decoder(dec_X, dec_state) # 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入 dec_X = Y.argmax(dim=2) pred = dec_X.squeeze(dim=0).type(torch.int32).item() # 保存注意力权重（稍后讨论） if save_attention_weights: attention_weight_seq.append(net.decoder.attention_weights) # 一旦序列结束词元被预测，输出序列的生成就完成了 if pred == tgt_vocab[\u0026#39;\u0026lt;eos\u0026gt;\u0026#39;]: break output_seq.append(pred) return \u0026#39; \u0026#39;.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq 预测序列的评估\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def bleu(pred_seq, label_seq, k): #@save \u0026#34;\u0026#34;\u0026#34;计算BLEU\u0026#34;\u0026#34;\u0026#34; pred_tokens, label_tokens = pred_seq.split(\u0026#39; \u0026#39;), label_seq.split(\u0026#39; \u0026#39;) len_pred, len_label = len(pred_tokens), len(label_tokens) score = math.exp(min(0, 1 - len_label / len_pred)) for n in range(1, k + 1): num_matches, label_subs = 0, collections.defaultdict(int) for i in range(len_label - n + 1): label_subs[\u0026#39; \u0026#39;.join(label_tokens[i: i + n])] += 1 for i in range(len_pred - n + 1): if label_subs[\u0026#39; \u0026#39;.join(pred_tokens[i: i + n])] \u0026gt; 0: num_matches += 1 label_subs[\u0026#39; \u0026#39;.join(pred_tokens[i: i + n])] -= 1 score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n)) return score engs = [\u0026#39;go .\u0026#39;, \u0026#34;i lost .\u0026#34;, \u0026#39;he\\\u0026#39;s calm .\u0026#39;, \u0026#39;i\\\u0026#39;m home .\u0026#39;] fras = [\u0026#39;va !\u0026#39;, \u0026#39;j\\\u0026#39;ai perdu .\u0026#39;, \u0026#39;il est calme .\u0026#39;, \u0026#39;je suis chez moi .\u0026#39;] for eng, fra in zip(engs, fras): translation, attention_weight_seq = predict_seq2seq( net, eng, src_vocab, tgt_vocab, num_steps, device) print(f\u0026#39;{eng} =\u0026gt; {translation}, bleu {bleu(translation, fra, k=2):.3f}\u0026#39;) # go . =\u0026gt; va !, bleu 1.000 # i lost . =\u0026gt; j\u0026#39;ai perdu ., bleu 1.000 # he\u0026#39;s calm . =\u0026gt; il est riche ., bleu 0.658 # i\u0026#39;m home . =\u0026gt; je suis en retard ?, bleu 0.447 束搜索 seq2seq 中使用了贪心搜索的策略，即将当前时刻预测概率最大的词输出\n但贪心并不一定最优\n前者（贪心）=0.048，后者=0.054\n穷举搜索：计算所有可能序列的概率，不可能实现 束搜索：n 个可选，保存最好的 k 个候选 时间复杂度：O(knT)\n句子越长概率越低，为了避免倾向于选过短的句子，在取 log 后乘上 L 项\n","date":"2024-09-25T00:00:00Z","permalink":"https://changle-liu.github.io/p/%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","title":"现代循环神经网络-动手学深度学习"},{"content":"动手学深度学习v2\n课程链接：https://courses.d2l.ai/zh-v2/\n序列模型 思路 在时间 t 观察到$ x_t $，得到 T 个不独立的随机变量 自回归模型：对见过的数据建模 马尔可夫模型：只和过去$ \\tau $个数据点有关 用 MLP 即可建模\n潜变量模型 引入潜变量$ h_t $表示过去的信息\n这使得 x 和 h 只与两个变量相关，方便建模\n代码 1 2 3 4 5 6 7 8 9 %matplotlib inline import torch from torch import nn from d2l import torch as d2l T = 1000 # 总共产生1000个点 time = torch.arange(1, T + 1, dtype = torch.float32) x = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,)) # T后面加单号，使T被识别为元组 d2l.plot(time, [x], \u0026#39;time\u0026#39;, \u0026#39;x\u0026#39;, xlim=[1, 1000], figsize=(6, 3)) 1 2 3 4 5 6 7 8 9 10 tau = 4 features = torch.zeros((T - tau, tau)) # (T - tau)行，每行tau个时间步 for i in range(tau): # 每次循环赋值1个时间步的数据 features[:, i] = x[i: T - tau + i] labels = x[tau:].reshape((-1, 1)) # reshape()将行数列数变为1 batch_size, n_train = 16, 600 # 只有前n_train个样本用于训练 train_iter = d2l.load_array((features[:n_train], labels[:n_train]), batch_size, is_train=True) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 初始化网络权重的函数 def init_weights(m): if type(m) == nn.Linear: nn.init.xavier_uniform_(m.weight) # 一个简单的多层感知机 def get_net(): net = nn.Sequential(nn.Linear(4, 10), nn.ReLU(), nn.Linear(10, 1)) net.apply(init_weights) return net # 平方损失。注意：MSELoss计算平方误差时不带系数1/2 loss = nn.MSELoss(reduction=\u0026#39;none\u0026#39;) # epoch 1, loss: 0.078787 # epoch 2, loss: 0.054550 # epoch 3, loss: 0.053713 # epoch 4, loss: 0.052182 # epoch 5, loss: 0.055492 单步预测效果较好\n1 2 3 4 5 onestep_preds = net(features) d2l.plot([time, time[tau:]], [x.detach().numpy(), onestep_preds.detach().numpy()], \u0026#39;time\u0026#39;, \u0026#39;x\u0026#39;, legend=[\u0026#39;data\u0026#39;, \u0026#39;1-step preds\u0026#39;], xlim=[1, 1000], figsize=(6, 3)) 604 之后如果使用自己的预测数据来继续预测，由于错误的累积，效果不好（绿线）\n已知 4 个点，分别预测未来 1、4、16、64 个点\n文本预处理 读取数据集 1 2 3 import collections import re from d2l import torch as d2l 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 d2l.DATA_HUB[\u0026#39;time_machine\u0026#39;] = (d2l.DATA_URL + \u0026#39;timemachine.txt\u0026#39;, \u0026#39;090b5e7e70c295757f55df93cb0a180b9691891a\u0026#39;) def read_time_machine(): #@save \u0026#34;\u0026#34;\u0026#34;将时间机器数据集加载到文本行的列表中\u0026#34;\u0026#34;\u0026#34; with open(d2l.download(\u0026#39;time_machine\u0026#39;), \u0026#39;r\u0026#39;) as f: lines = f.readlines() # re.sub(\u0026#39;[^A-Za-z]+\u0026#39;, \u0026#39; \u0026#39;, line): 正则表达式，将line中非字母字符替换成空格 # .strip(): 去掉字符串两端的空白字符 # .lower(): 转换成小写 return [re.sub(\u0026#39;[^A-Za-z]+\u0026#39;, \u0026#39; \u0026#39;, line).strip().lower() for line in lines] lines = read_time_machine() print(f\u0026#39;# 文本总行数: {len(lines)}\u0026#39;) print(lines[0]) print(lines[10]) 词元化 1 2 3 4 5 6 7 8 9 10 11 12 def tokenize(lines, token=\u0026#39;word\u0026#39;): #@save \u0026#34;\u0026#34;\u0026#34;将文本行拆分为单词或字符词元\u0026#34;\u0026#34;\u0026#34; if token == \u0026#39;word\u0026#39;: return [line.split() for line in lines] elif token == \u0026#39;char\u0026#39;: return [list(line) for line in lines] else: print(\u0026#39;错误：未知词元类型：\u0026#39; + token) tokens = tokenize(lines) print(tokens[0]) # [\u0026#39;the\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;machine\u0026#39;, \u0026#39;by\u0026#39;, \u0026#39;h\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;wells\u0026#39;] 词表 构建一个字典，将每个 token 映射到数字索引上\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class Vocab: #@save \u0026#34;\u0026#34;\u0026#34;文本词表\u0026#34;\u0026#34;\u0026#34; # min_freq: 出现频率超过该值则加入字典 def __init__(self, tokens=None, min_freq=0, reserved_tokens=None): if tokens is None: tokens = [] if reserved_tokens is None: reserved_tokens = [] # 按出现频率排序 counter = count_corpus(tokens) # key=lambda x: x[1] 每个(key, value)元组的第二个元素 self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True) # 未知词元的索引为0 self.idx_to_token = [\u0026#39;\u0026lt;unk\u0026gt;\u0026#39;] + reserved_tokens self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)} # 将每个token加入字典 for token, freq in self._token_freqs: if freq \u0026lt; min_freq: break if token not in self.token_to_idx: self.idx_to_token.append(token) self.token_to_idx[token] = len(self.idx_to_token) - 1 def __len__(self): return len(self.idx_to_token) def __getitem__(self, tokens): # 判断是否是列表或元组 if not isinstance(tokens, (list, tuple)): return self.token_to_idx.get(tokens, self.unk) return [self.__getitem__(token) for token in tokens] def to_tokens(self, indices): if not isinstance(indices, (list, tuple)): return self.idx_to_token[indices] return [self.idx_to_token[index] for index in indices] @property def unk(self): # 未知词元的索引为0 return 0 @property def token_freqs(self): return self._token_freqs def count_corpus(tokens): #@save \u0026#34;\u0026#34;\u0026#34;统计词元的频率\u0026#34;\u0026#34;\u0026#34; # 这里的tokens是1D列表或2D列表 if len(tokens) == 0 or isinstance(tokens[0], list): # 将词元列表展平成一个列表 tokens = [token for line in tokens for token in line] return collections.Counter(tokens) 1 2 3 4 5 6 7 8 9 10 11 12 # 打印验证最高频词 vocab = Vocab(tokens) print(list(vocab.token_to_idx.items())[:10]) # [(\u0026#39;\u0026lt;unk\u0026gt;\u0026#39;, 0), (\u0026#39;the\u0026#39;, 1), (\u0026#39;i\u0026#39;, 2), (\u0026#39;and\u0026#39;, 3), (\u0026#39;of\u0026#39;, 4), (\u0026#39;a\u0026#39;, 5), (\u0026#39;to\u0026#39;, 6), (\u0026#39;was\u0026#39;, 7), (\u0026#39;in\u0026#39;, 8), (\u0026#39;that\u0026#39;, 9)] for i in [0, 10]: print(\u0026#39;文本:\u0026#39;, tokens[i]) print(\u0026#39;索引:\u0026#39;, vocab[tokens[i]]) # 文本: [\u0026#39;the\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;machine\u0026#39;, \u0026#39;by\u0026#39;, \u0026#39;h\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;wells\u0026#39;] # 索引: [1, 19, 50, 40, 2183, 2184, 400] # 文本: [\u0026#39;twinkled\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;usually\u0026#39;, \u0026#39;pale\u0026#39;, \u0026#39;face\u0026#39;, \u0026#39;was\u0026#39;, \u0026#39;flushed\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;animated\u0026#39;, \u0026#39;the\u0026#39;] # 索引: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1] 功能整合 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def load_corpus_time_machine(max_tokens=-1): #@save \u0026#34;\u0026#34;\u0026#34;返回时光机器数据集的词元索引列表和词表\u0026#34;\u0026#34;\u0026#34; lines = read_time_machine() tokens = tokenize(lines, \u0026#39;char\u0026#39;) vocab = Vocab(tokens) # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落， # 所以将所有文本行展平到一个列表中 corpus = [vocab[token] for line in tokens for token in line] if max_tokens \u0026gt; 0: corpus = corpus[:max_tokens] return corpus, vocab corpus, vocab = load_corpus_time_machine() # vocab为记录字符的字典，corpus为字符对应索引组成的对应文本向量 len(corpus), len(vocab) # (170580, 28) 语言模型和数据集 语言模型 给定文本序列$ x_1, \u0026hellip;x_T $，语言模型目标是估计联合概率$ p(x_1, \u0026hellip;, x_T) $\n当序列很长时，可使用马尔可夫假设\n三元语法：\n自然语言统计 1 2 3 import random import torch from d2l import torch as d2l 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 tokens = d2l.tokenize(d2l.read_time_machine()) # 因为每个文本行不一定是一个句子或一个段落，因此我们把所有文本行拼接到一起 corpus = [token for line in tokens for token in line] vocab = d2l.Vocab(corpus) vocab.token_freqs[:10] # [(\u0026#39;the\u0026#39;, 2261), # (\u0026#39;i\u0026#39;, 1267), # (\u0026#39;and\u0026#39;, 1245), # (\u0026#39;of\u0026#39;, 1155), # (\u0026#39;a\u0026#39;, 816), # (\u0026#39;to\u0026#39;, 695), # (\u0026#39;was\u0026#39;, 552), # (\u0026#39;in\u0026#39;, 541), # (\u0026#39;that\u0026#39;, 443), # (\u0026#39;my\u0026#39;, 440)] 最流行的词通常没有什么意义，被称为停用词\n1 2 3 4 # 画出词频图 freqs = [freq for token, freq in vocab.token_freqs] d2l.plot(freqs, xlabel=\u0026#39;token: x\u0026#39;, ylabel=\u0026#39;frequency: n(x)\u0026#39;, xscale=\u0026#39;log\u0026#39;, yscale=\u0026#39;log\u0026#39;) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 将两个corpus错位后排成组，查看二元语法中最高频的词 bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])] bigram_vocab = d2l.Vocab(bigram_tokens) bigram_vocab.token_freqs[:10] # [((\u0026#39;of\u0026#39;, \u0026#39;the\u0026#39;), 309), # ((\u0026#39;in\u0026#39;, \u0026#39;the\u0026#39;), 169), # ((\u0026#39;i\u0026#39;, \u0026#39;had\u0026#39;), 130), # ((\u0026#39;i\u0026#39;, \u0026#39;was\u0026#39;), 112), # ((\u0026#39;and\u0026#39;, \u0026#39;the\u0026#39;), 109), # ((\u0026#39;the\u0026#39;, \u0026#39;time\u0026#39;), 102), # ((\u0026#39;it\u0026#39;, \u0026#39;was\u0026#39;), 99), # ((\u0026#39;to\u0026#39;, \u0026#39;the\u0026#39;), 85), # ((\u0026#39;as\u0026#39;, \u0026#39;i\u0026#39;), 78), # ((\u0026#39;of\u0026#39;, \u0026#39;a\u0026#39;), 73)] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 同样方法统计三元语法的词频 trigram_tokens = [triple for triple in zip( corpus[:-2], corpus[1:-1], corpus[2:])] trigram_vocab = d2l.Vocab(trigram_tokens) trigram_vocab.token_freqs[:10] # [((\u0026#39;the\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;traveller\u0026#39;), 59), # ((\u0026#39;the\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;machine\u0026#39;), 30), # ((\u0026#39;the\u0026#39;, \u0026#39;medical\u0026#39;, \u0026#39;man\u0026#39;), 24), # ((\u0026#39;it\u0026#39;, \u0026#39;seemed\u0026#39;, \u0026#39;to\u0026#39;), 16), # ((\u0026#39;it\u0026#39;, \u0026#39;was\u0026#39;, \u0026#39;a\u0026#39;), 15), # ((\u0026#39;here\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;there\u0026#39;), 15), # ((\u0026#39;seemed\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;me\u0026#39;), 14), # ((\u0026#39;i\u0026#39;, \u0026#39;did\u0026#39;, \u0026#39;not\u0026#39;), 14), # ((\u0026#39;i\u0026#39;, \u0026#39;saw\u0026#39;, \u0026#39;the\u0026#39;), 13), # ((\u0026#39;i\u0026#39;, \u0026#39;began\u0026#39;, \u0026#39;to\u0026#39;), 13)] 1 2 3 4 5 6 # 画出曲线 bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs] trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs] d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel=\u0026#39;token: x\u0026#39;, ylabel=\u0026#39;frequency: n(x)\u0026#39;, xscale=\u0026#39;log\u0026#39;, yscale=\u0026#39;log\u0026#39;, legend=[\u0026#39;unigram\u0026#39;, \u0026#39;bigram\u0026#39;, \u0026#39;trigram\u0026#39;]) 读取长序列数据 随机采样 在 0 到 T-1 之间随机选取起始位置，每 T 个 token 分成一个 batch\n这样可以减小切分处序列的影响\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def seq_data_iter_random(corpus, batch_size, num_steps): #@save \u0026#34;\u0026#34;\u0026#34;使用随机抽样生成一个小批量子序列\u0026#34;\u0026#34;\u0026#34; # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1 corpus = corpus[random.randint(0, num_steps - 1):] # 减去1，保证最后一个序列有label num_subseqs = (len(corpus) - 1) // num_steps # 长度为num_steps的子序列的起始索引 initial_indices = list(range(0, num_subseqs * num_steps, num_steps)) # 在随机抽样的迭代过程中， # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻 random.shuffle(initial_indices) def data(pos): # 返回从pos位置开始的长度为num_steps的序列 return corpus[pos: pos + num_steps] num_batches = num_subseqs // batch_size for i in range(0, batch_size * num_batches, batch_size): # 在这里，initial_indices包含子序列的随机起始索引 initial_indices_per_batch = initial_indices[i: i + batch_size] X = [data(j) for j in initial_indices_per_batch] Y = [data(j + 1) for j in initial_indices_per_batch] yield torch.tensor(X), torch.tensor(Y) 顺序分区 保证两个相邻的小批量中的子序列在原始序列上也是相邻的\n1 2 3 4 5 6 7 8 9 10 11 12 13 def seq_data_iter_sequential(corpus, batch_size, num_steps): #@save \u0026#34;\u0026#34;\u0026#34;使用顺序分区生成一个小批量子序列\u0026#34;\u0026#34;\u0026#34; # 从随机偏移量开始划分序列 offset = random.randint(0, num_steps) num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size Xs = torch.tensor(corpus[offset: offset + num_tokens]) Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens]) Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1) num_batches = Xs.shape[1] // num_steps for i in range(0, num_steps * num_batches, num_steps): X = Xs[:, i: i + num_steps] Y = Ys[:, i: i + num_steps] yield X, Y 1 2 3 4 5 6 7 8 9 10 11 12 13 14 for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5): print(\u0026#39;X: \u0026#39;, X, \u0026#39;\\nY:\u0026#39;, Y) # X: tensor([[ 0, 1, 2, 3, 4], # [17, 18, 19, 20, 21]]) # Y: tensor([[ 1, 2, 3, 4, 5], # [18, 19, 20, 21, 22]]) # X: tensor([[ 5, 6, 7, 8, 9], # [22, 23, 24, 25, 26]]) # Y: tensor([[ 6, 7, 8, 9, 10], # [23, 24, 25, 26, 27]]) # X: tensor([[10, 11, 12, 13, 14], # [27, 28, 29, 30, 31]]) # Y: tensor([[11, 12, 13, 14, 15], # [28, 29, 30, 31, 32]]) 包装到类：\n1 2 3 4 5 6 7 8 9 10 11 12 class SeqDataLoader: #@save \u0026#34;\u0026#34;\u0026#34;加载序列数据的迭代器\u0026#34;\u0026#34;\u0026#34; def __init__(self, batch_size, num_steps, use_random_iter, max_tokens): if use_random_iter: self.data_iter_fn = d2l.seq_data_iter_random else: self.data_iter_fn = d2l.seq_data_iter_sequential self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens) self.batch_size, self.num_steps = batch_size, num_steps def __iter__(self): return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps) 1 2 3 4 5 6 def load_data_time_machine(batch_size, num_steps, #@save use_random_iter=False, max_tokens=10000): \u0026#34;\u0026#34;\u0026#34;返回时光机器数据集的迭代器和词表\u0026#34;\u0026#34;\u0026#34; data_iter = SeqDataLoader( batch_size, num_steps, use_random_iter, max_tokens) return data_iter, data_iter.vocab 循环神经网络 RNN 相比于 MLP 加入了隐变量\n困惑度 Perplexity\n衡量语言模型好坏时可以用平均交叉熵，平均交叉熵时一个序列中所有的 n 个词元的交叉熵损失的平均值\nNLP 中使用困惑度来衡量：$ exp(\\pi) $\n1 表示完美，最坏情况是无穷大\n梯度裁剪\nT 时间步上的梯度在反向传播时会产生长度为 O(T) 的矩阵乘法链，导致数值不稳定\n梯度裁剪来防止梯度爆炸\n梯度长度超过$ \\theta $时，拖影回长度$ \\theta $\n确保了梯度不会大于 1\nRNN 应用\n从零实现 1 2 3 4 5 6 7 8 9 10 %matplotlib inline import math import torch from torch import nn from torch.nn import functional as F from d2l import torch as d2l # 定义批量大小和步长 batch_size, num_steps = 32, 35 train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps) 独热编码\n通过独热编码可以将词元的数字索引转换成特征向量\n1 2 3 4 5 6 # 0 和 2 的独热变量 F.one_hot(torch.tensor([0, 2]), len(vocab)) # tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, # 0, 0, 0, 0], # [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, # 0, 0, 0, 0]]) 采样的小批量数据形状：（批量大小，时间步数） 转置输入的维度，以便获得形状为（时间步数，批量大小，词表大小）的输出 之所以这样做，是为了方便通过外层的维度，一步一步更新小批量数据的隐状态 这样一来 X[i, :, :] 代表 T(i)的状态 1 2 3 X = torch.arange(10).reshape((2, 5)) F.one_hot(X.T, 28).shape # torch.Size([5, 2, 28]) 初始化模型参数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def get_params(vocab_size, num_hiddens, device): # 输入和输出都来自词表，因此和词表大小相同 num_inputs = num_outputs = vocab_size # 之后频繁用到的初始化函数 def normal(shape): return torch.randn(size=shape, device=device) * 0.01 # 隐藏层参数 # 输入变量x到隐藏层变量的矩阵 W_xh = normal((num_inputs, num_hiddens)) # 前一个隐藏变量到下一个隐藏变量 W_hh = normal((num_hiddens, num_hiddens)) b_h = torch.zeros(num_hiddens, device=device) # 输出层参数 W_hq = normal((num_hiddens, num_outputs)) b_q = torch.zeros(num_outputs, device=device) # 附加梯度 params = [W_xh, W_hh, b_h, W_hq, b_q] for param in params: param.requires_grad_(True) return params 循环神经网络模型\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # 初始化隐藏状态 def init_rnn_state(batch_size, num_hiddens, device): # 返回一个元组，后续隐状态可能包含多个变量 return (torch.zeros((batch_size, num_hiddens), device=device), ) # 定义一个时间步内计算隐状态和输出 def rnn(inputs, state, params): # inputs的形状：(时间步数量，批量大小，词表大小) W_xh, W_hh, b_h, W_hq, b_q = params H, = state outputs = [] # 之前取了转置，这里可以直接迭代，每次取出一个时间步的 X # X的形状：(批量大小，词表大小) for X in inputs: # 选用tanh作为激活函数 H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h) Y = torch.mm(H, W_hq) + b_q outputs.append(Y) # 输出沿时间步维度拼接 return torch.cat(outputs, dim=0), (H,) class RNNModelScratch: #@save \u0026#34;\u0026#34;\u0026#34;从零开始实现的循环神经网络模型\u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn): self.vocab_size, self.num_hiddens = vocab_size, num_hiddens self.params = get_params(vocab_size, num_hiddens, device) self.init_state, self.forward_fn = init_state, forward_fn def __call__(self, X, state): X = F.one_hot(X.T, self.vocab_size).type(torch.float32) return self.forward_fn(X, state, self.params) def begin_state(self, batch_size, device): return self.init_state(batch_size, self.num_hiddens, device) # 检验输出形状 num_hiddens = 512 net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn) state = net.begin_state(X.shape[0], d2l.try_gpu()) Y, new_state = net(X.to(d2l.try_gpu()), state) Y.shape, len(new_state), new_state[0].shape # Y.shape = (批量大小*时间步数, 对下一步的预测向量) # (torch.Size([10, 28]), 1, torch.Size([2, 512])) 预测\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def predict_ch8(prefix, num_preds, net, vocab, device): #@save \u0026#34;\u0026#34;\u0026#34;在prefix后面生成新字符\u0026#34;\u0026#34;\u0026#34; # 生成初始的隐藏状态 state = net.begin_state(batch_size=1, device=device) outputs = [vocab[prefix[0]]] # 取出output中最后一个词作为input get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1)) for y in prefix[1:]: # 预热期：将prefix存入output并更新state _, state = net(get_input(), state) outputs.append(vocab[y]) # 直接把真实值放入output # 接下来开始对已有数据的后续数据进行预测 for _ in range(num_preds): # 预测num_preds步 y, state = net(get_input(), state) # 选取分类后最大值的坐标，这个坐标便是独热编码对应值 outputs.append(int(y.argmax(dim=1).reshape(1))) # 将output中编码在vocab中查找并返回 return \u0026#39;\u0026#39;.join([vocab.idx_to_token[i] for i in outputs]) 梯度裁剪\n1 2 3 4 5 6 7 8 9 10 11 12 def grad_clipping(net, theta): #@save \u0026#34;\u0026#34;\u0026#34;裁剪梯度\u0026#34;\u0026#34;\u0026#34; # 将所有层的参数都取出 if isinstance(net, nn.Module): params = [p for p in net.parameters() if p.requires_grad] else: params = net.params # 求所有层参数组成的向量的二阶范数 norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params)) if norm \u0026gt; theta: for param in params: param.grad[:] *= theta / norm 训练\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter): # use_random_iter: 采用不同的采样方法（随机、顺序分区）将会导致隐状态初始化差异 \u0026#34;\u0026#34;\u0026#34;训练网络一个迭代周期（定义见第8章）\u0026#34;\u0026#34;\u0026#34; state, timer = None, d2l.Timer() metric = d2l.Accumulator(2) # 训练损失之和,词元数量 for X, Y in train_iter: # 如果随机采样，在每个batch开始前都会重新初始化state为0 if state is None or use_random_iter: # 在第一次迭代或使用随机抽样时初始化state state = net.begin_state(batch_size=X.shape[0], device=device) else: if isinstance(net, nn.Module) and not isinstance(state, tuple): # state对于nn.GRU是个张量 # 由于隐状态计算依赖于先前批量数据，反复累计会使梯度计算变得复杂 # 为了降低计算量，在每轮训练开始前，先把隐状态先前带有的梯度分离 # 只专注于该轮的梯度计算 state.detach_() else: # state对于nn.LSTM或对于我们从零开始实现的模型是个张量 for s in state: s.detach_() y = Y.T.reshape(-1) X, y = X.to(device), y.to(device) y_hat, state = net(X, state) # 对loss来说y就是(批量大小*时间步数)大小的样本，因此直接拉成一条向量 # 在rnn()中，y_hat也被沿dim=0时间步维度拉成了一条向量 l = loss(y_hat, y.long()).mean() if isinstance(updater, torch.optim.Optimizer): # 如果用的是torch.optim.Optimizer的updater，需要梯度置零，因为默认累加 updater.zero_grad() l.backward() grad_clipping(net, 1) # 梯度剪裁 updater.step() else: l.backward() grad_clipping(net, 1) # 因为已经调用了mean函数 updater(batch_size=1) metric.add(l * y.numel(), y.numel()) # metric通常包含两个元素：元素存储总损失、元素储存样本数量，用来在训练结束时计算平均损失 # 返回困惑度、每秒处理样本数 return math.exp(metric[0] / metric[1]), metric[1] / timer.stop() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def train_ch8(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False): \u0026#34;\u0026#34;\u0026#34;训练模型（定义见第8章）\u0026#34;\u0026#34;\u0026#34; loss = nn.CrossEntropyLoss() animator = d2l.Animator(xlabel=\u0026#39;epoch\u0026#39;, ylabel=\u0026#39;perplexity\u0026#39;, legend=[\u0026#39;train\u0026#39;], xlim=[10, num_epochs]) # 初始化 if isinstance(net, nn.Module): updater = torch.optim.SGD(net.parameters(), lr) else: updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size) predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device) # 训练和预测 for epoch in range(num_epochs): ppl, speed = train_epoch_ch8( net, train_iter, loss, updater, device, use_random_iter) if (epoch + 1) % 10 == 0: print(predict(\u0026#39;time traveller\u0026#39;)) animator.add(epoch + 1, [ppl]) print(f\u0026#39;困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}\u0026#39;) print(predict(\u0026#39;time traveller\u0026#39;)) print(predict(\u0026#39;traveller\u0026#39;)) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #@save def train_ch8(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False): \u0026#34;\u0026#34;\u0026#34;训练模型（定义见第8章）\u0026#34;\u0026#34;\u0026#34; loss = nn.CrossEntropyLoss() animator = d2l.Animator(xlabel=\u0026#39;epoch\u0026#39;, ylabel=\u0026#39;perplexity\u0026#39;, legend=[\u0026#39;train\u0026#39;], xlim=[10, num_epochs]) # 初始化 if isinstance(net, nn.Module): updater = torch.optim.SGD(net.parameters(), lr) else: updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size) predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device) # 训练和预测 for epoch in range(num_epochs): ppl, speed = train_epoch_ch8( net, train_iter, loss, updater, device, use_random_iter) if (epoch + 1) % 10 == 0: print(predict(\u0026#39;time traveller\u0026#39;)) animator.add(epoch + 1, [ppl]) print(f\u0026#39;困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}\u0026#39;) print(predict(\u0026#39;time traveller\u0026#39;)) print(predict(\u0026#39;traveller\u0026#39;)) 结果\n1 2 num_epochs, lr = 500, 1 train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu()) 1 2 3 4 5 # 随机抽样 net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn) train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(), use_random_iter=True) 简洁实现 1 2 3 4 5 6 7 import torch from torch import nn from torch.nn import functional as F from d2l import torch as d2l batch_size, num_steps = 32, 35 train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps) 定义模型\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 num_hiddens = 256 rnn_layer = nn.RNN(len(vocab), num_hiddens) # 只包含隐藏的循环层，需要手动创建一个单独的输出层 # 初始化隐状态 state = torch.zeros((1, batch_size, num_hiddens)) state.shape # torch.Size([1, 32, 256]) X = torch.rand(size=(num_steps, batch_size, len(vocab))) # 这里的 Y 是最后一个隐藏层而不是输出，因此维度是256而不是len(vocab) Y, state_new = rnn_layer(X, state) Y.shape, state_new.shape (torch.Size([35, 32, 256]), torch.Size([1, 32, 256])) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class RNNModel(nn.Module): \u0026#34;\u0026#34;\u0026#34;循环神经网络模型\u0026#34;\u0026#34;\u0026#34; def __init__(self, rnn_layer, vocab_size, **kwargs): super(RNNModel, self).__init__(**kwargs) self.rnn = rnn_layer self.vocab_size = vocab_size self.num_hiddens = self.rnn.hidden_size # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1 # rnn_layer中只包含隐藏层，这里需要加上输出层 if not self.rnn.bidirectional: self.num_directions = 1 self.linear = nn.Linear(self.num_hiddens, self.vocab_size) else: self.num_directions = 2 self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size) def forward(self, inputs, state): # 先转换成长整型再进行独热编码 X = F.one_hot(inputs.T.long(), self.vocab_size) # 再转换回float32 X = X.to(torch.float32) Y, state = self.rnn(X, state) # 全连接层首先将Y的形状由(时间步数,批量大小,隐藏单元数)改为(时间步数*批量大小,隐藏单元数) # 它的输出形状是(时间步数*批量大小,词表大小) # 第一个-1代表该维度大小由pytorch自动计算 output = self.linear(Y.reshape((-1, Y.shape[-1]))) return output, state def begin_state(self, device, batch_size=1): if not isinstance(self.rnn, nn.LSTM): # nn.GRU以张量作为隐状态 return torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device) else: # nn.LSTM以元组作为隐状态 return (torch.zeros(( self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device), torch.zeros(( self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device)) ","date":"2024-09-21T00:00:00Z","permalink":"https://changle-liu.github.io/p/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","title":"循环神经网络-动手学深度学习"},{"content":"动手学深度学习v2\n课程链接：https://courses.d2l.ai/zh-v2/\n多 GPU 训练（待补充） 从零实现 简洁实现 分布式训练 假如数据集有 n 个类，那么批量大小 batch_size 最好不要超过 10n：\nbatchsize达到一定程度，每个batch内的样本的多样性不会比之前有多大增长，对梯度的贡献也不会比之前的batch大多少，但是大的batchsize会带来更多的训练时间，就造成了训练有效性下降。\n假设总共有10000个样本 1. 如果一个batch是100, 那么一个epoch可以迭代100次梯度; 2. 如果一个batch是1000, 那么一个epoch只能迭代10次梯度, 如果想要收敛, 意味着需要更多epoch.\n","date":"2024-09-13T00:00:00Z","permalink":"https://changle-liu.github.io/p/%E8%AE%A1%E7%AE%97%E6%80%A7%E8%83%BD-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","title":"计算性能-动手学深度学习"},{"content":"动手学深度学习v2\n课程链接：https://courses.d2l.ai/zh-v2/\n深度卷积神经网络 AlexNet AlexNet 相比于LeNet：\n更深更宽 激活函数从sigmoid变成ReLU（减缓梯度消失） 隐藏全连接层后加入了丢弃层 数据增强 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 import torch from torch import nn from d2l import torch as d2l net = nn.Sequential( # 这里使用一个11*11的更大窗口来捕捉对象。 # 同时，步幅为4，以减少输出的高度和宽度。 # 另外，输出通道的数目远大于LeNet nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数 nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), # 使用三个连续的卷积层和较小的卷积窗口。 # 除了最后的卷积层，输出通道的数量进一步增加。 # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度 nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(), # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合 nn.Linear(6400, 4096), nn.ReLU(), nn.Dropout(p=0.5), nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5), # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000 nn.Linear(4096, 10)) # 输入224*224的单通道数据，观察每层输出的形状 X = torch.randn(1, 1, 224, 224) for layer in net: X=layer(X) print(layer.__class__.__name__,\u0026#39;output shape:\\t\u0026#39;,X.shape) # Conv2d output shape:\ttorch.Size([1, 96, 54, 54]) # ReLU output shape:\ttorch.Size([1, 96, 54, 54]) # MaxPool2d output shape:\ttorch.Size([1, 96, 26, 26]) # Conv2d output shape:\ttorch.Size([1, 256, 26, 26]) # ReLU output shape:\ttorch.Size([1, 256, 26, 26]) # MaxPool2d output shape:\ttorch.Size([1, 256, 12, 12]) # Conv2d output shape:\ttorch.Size([1, 384, 12, 12]) # ReLU output shape:\ttorch.Size([1, 384, 12, 12]) # Conv2d output shape:\ttorch.Size([1, 384, 12, 12]) # ReLU output shape:\ttorch.Size([1, 384, 12, 12]) # Conv2d output shape:\ttorch.Size([1, 256, 12, 12]) # ReLU output shape:\ttorch.Size([1, 256, 12, 12]) # MaxPool2d output shape:\ttorch.Size([1, 256, 5, 5]) # Flatten output shape:\ttorch.Size([1, 6400]) # Linear output shape:\ttorch.Size([1, 4096]) # ReLU output shape:\ttorch.Size([1, 4096]) # Dropout output shape:\ttorch.Size([1, 4096]) # Linear output shape:\ttorch.Size([1, 4096]) # ReLU output shape:\ttorch.Size([1, 4096]) # Dropout output shape:\ttorch.Size([1, 4096]) # Linear output shape:\ttorch.Size([1, 10]) # 训练 # 使用相较以前更小的学习率 lr, num_epochs = 0.01, 10 d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu()) # loss 0.331, train acc 0.878, test acc 0.883 # 3941.8 examples/sec on cuda:0 # \u0026lt;Figure size 252x180 with 1 Axes\u0026gt; 使用块的网络 VGG 将AlexNet的多层结构替换为VGG块 VGG: 33卷积核、填充为1（保持高度和宽度）的卷积层，和22汇聚窗口、步幅为2（每个块后的分辨率减半）的最大汇聚层 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import torch from torch import nn from d2l import torch as d2l def vgg_block(num_convs, in_channels, out_channels): layers = [] for _ in range(num_convs): layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)) layers.append(nn.ReLU()) in_channels = out_channels layers.append(nn.MaxPool2d(kernel_size=2,stride=2)) return nn.Sequential(*layers) VGG-11: 8个卷积层和3个全连接层\n原始VGG网络有5个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。 第一个模块有64个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到512。由于该网络使用8个卷积层和3个全连接层，因此它通常被称为VGG-11。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512)) def vgg(conv_arch): conv_blks = [] in_channels = 1 # 卷积层部分 for (num_convs, out_channels) in conv_arch: conv_blks.append(vgg_block(num_convs, in_channels, out_channels)) in_channels = out_channels return nn.Sequential( *conv_blks, nn.Flatten(), # 全连接层部分 nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5), nn.Linear(4096, 10)) net = vgg(conv_arch) # 构造224*224单通道数据样本 X = torch.randn(size=(1, 1, 224, 224)) for blk in net: X = blk(X) print(blk.__class__.__name__,\u0026#39;output shape:\\t\u0026#39;,X.shape) # Sequential output shape: torch.Size([1, 64, 112, 112]) # Sequential output shape: torch.Size([1, 128, 56, 56]) # Sequential output shape: torch.Size([1, 256, 28, 28]) # Sequential output shape: torch.Size([1, 512, 14, 14]) # Sequential output shape: torch.Size([1, 512, 7, 7]) # Flatten output shape: torch.Size([1, 25088]) # Linear output shape: torch.Size([1, 4096]) # ReLU output shape: torch.Size([1, 4096]) # Dropout output shape: torch.Size([1, 4096]) # Linear output shape: torch.Size([1, 4096]) # ReLU output shape: torch.Size([1, 4096]) # Dropout output shape: torch.Size([1, 4096]) # Linear output shape: torch.Size([1, 10]) 网络中的网络 NiN 全连接层的问题：卷积层参数较少，但卷积层后的第一个全连接层参数过多\n核心思想：\nNiN块：卷积层+2 个 11 卷积层， 11 卷积层前后的 ReLU 对每个像素增加了非线性性 模型最后端采用全局平均池化层，来代替 VGG 和 AlexNet 中的全连接层 =\u0026gt; 不容易过拟合，更少的参数 1 2 3 4 5 6 7 8 9 10 11 12 import torch from torch import nn from d2l import torch as d2l # 定义：NiN块 def nin_block(in_channels, out_channels, kernel_size, strides, padding): return nn.Sequential( # 直接代入输入的参数 nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding), nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 net = nn.Sequential( # 灰度图输入通道为1 nin_block(1, 96, kernel_size=11, strides=4, padding=0), nn.MaxPool2d(3, stride=2), nin_block(96, 256, kernel_size=5, strides=1, padding=2), nn.MaxPool2d(3, stride=2), nin_block(256, 384, kernel_size=3, strides=1, padding=1), nn.MaxPool2d(3, stride=2), nn.Dropout(0.5), # 标签类别数是10（Fashion_MNIST） nin_block(384, 10, kernel_size=3, strides=1, padding=1), # 高宽都变为1 nn.AdaptiveAvgPool2d((1, 1)), # 将四维的输出转成二维的输出，其形状为(批量大小,10) nn.Flatten()) # 查看输出形状 X = torch.rand(size=(1, 1, 224, 224)) for layer in net: X = layer(X) print(layer.__class__.__name__,\u0026#39;output shape:\\t\u0026#39;, X.shape) # Sequential output shape: torch.Size([1, 96, 54, 54]) # MaxPool2d output shape: torch.Size([1, 96, 26, 26]) # Sequential output shape: torch.Size([1, 256, 26, 26]) # MaxPool2d output shape: torch.Size([1, 256, 12, 12]) # Sequential output shape: torch.Size([1, 384, 12, 12]) # MaxPool2d output shape: torch.Size([1, 384, 5, 5]) # Dropout output shape: torch.Size([1, 384, 5, 5]) # Sequential output shape: torch.Size([1, 10, 5, 5]) # AdaptiveAvgPool2d output shape: torch.Size([1, 10, 1, 1]) # Flatten output shape: torch.Size([1, 10]) # 训练 lr, num_epochs, batch_size = 0.1, 10, 128 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu()) 含并行连结的网络 GoogLeNet Inception 块 输入和输出的宽高相等，但通道数改变 白色快用来改变通道数，蓝色块用来抽取信息 和 33 或 55 卷积层相比，Inception 块的参数个数更少，计算复杂度更低 由后续各种变种 V2、V3、V4 GoogLeNet Stage 1 \u0026amp; 2 GoogLeNet 使用了更小的卷积层，最终的宽高更大，从而之后可以使用更深的网络\nStage 3 通道分配基本无规律可总结\nStage 4 \u0026amp; 5 代码实现 7.4. 含并行连结的网络（GoogLeNet） — 动手学深度学习 2.0.0 documentation\n照着网络敲代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 import torch from torch import nn from torch.nn import functional as F from d2l import torch as d2l class Inception(nn.Module): # c1--c4是每条路径的输出通道数 def __init__(self, in_channels, c1, c2, c3, c4, **kwargs): super(Inception, self).__init__(**kwargs) # 线路1，单1x1卷积层 self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1) # 线路2，1x1卷积层后接3x3卷积层 self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1) self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1) # 线路3，1x1卷积层后接5x5卷积层 self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1) self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2) # 线路4，3x3最大汇聚层后接1x1卷积层 self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1) def forward(self, x): p1 = F.relu(self.p1_1(x)) p2 = F.relu(self.p2_2(F.relu(self.p2_1(x)))) p3 = F.relu(self.p3_2(F.relu(self.p3_1(x)))) p4 = F.relu(self.p4_2(self.p4_1(x))) # 在通道维度上连结输出 return torch.cat((p1, p2, p3, p4), dim=1) b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1), nn.ReLU(), nn.Conv2d(64, 192, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32), Inception(256, 128, (128, 192), (32, 96), 64), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64), Inception(512, 160, (112, 224), (24, 64), 64), Inception(512, 128, (128, 256), (24, 64), 64), Inception(512, 112, (144, 288), (32, 64), 64), Inception(528, 256, (160, 320), (32, 128), 128), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128), Inception(832, 384, (192, 384), (48, 128), 128), nn.AdaptiveAvgPool2d((1,1)), nn.Flatten()) net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10)) X = torch.rand(size=(1, 1, 96, 96)) for layer in net: X = layer(X) print(layer.__class__.__name__,\u0026#39;output shape:\\t\u0026#39;, X.shape) # Sequential output shape: torch.Size([1, 64, 24, 24]) # Sequential output shape: torch.Size([1, 192, 12, 12]) # Sequential output shape: torch.Size([1, 480, 6, 6]) # Sequential output shape: torch.Size([1, 832, 3, 3]) # Sequential output shape: torch.Size([1, 1024]) # Linear output shape: torch.Size([1, 10]) lr, num_epochs, batch_size = 0.1, 10, 128 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu()) 批量归一化 核心思想 反向传递时，数据由上至下传递。梯度由于相乘，越靠下梯度越小。因此底层的变化会导致上层的数据跟着变，需要重新学习，导致收敛变慢 能否在学习底部的时候尽量避免顶部的变化？ 变化是因为每层方差和均值不同，考虑将不同层的不同位置的小批量的分布固定 （$ \\epsilon $确保$ \\sigma_B $不为 0）\n批量归一化层 作用在全连接层和卷积层的输出上，激活函数前：批量归一化是线性变化 或作用在全连接层和卷积层的输入上 对全连接层的作用是在特征维度上，对卷积层的作用在通道维上 特别是 1*1 的卷积核，就是将所有的像素当成样本来计算均值和方差，通道维可以看作是卷积层的特征维 作用 最初想法：减少内部协变量转移（用今天的数据拟合明天） 后续指出：在每个小批量中加入噪音来控制模型复杂度（选取小批量时是随机选取的，因此方差和均值也可看作是随机的） 因此没必要和丢弃法 dropout 混合使用 总结：\n固定小批量中的均值和方差，然后学习出适合的偏移和缩放 可以加快收敛速度，但一般不改变模型精度 代码实现 从零实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 import torch from torch import nn from d2l import torch as d2l def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum): # 通过is_grad_enabled来判断当前模式是训练模式还是预测模式 if not torch.is_grad_enabled(): # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差 X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps) else: assert len(X.shape) in (2, 4) if len(X.shape) == 2: # 使用全连接层的情况，计算特征维上的均值和方差 mean = X.mean(dim=0) var = ((X - mean) ** 2).mean(dim=0) else: # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。 # 这里我们需要保持X的形状以便后面可以做广播运算 mean = X.mean(dim=(0, 2, 3), keepdim=True) var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True) # 训练模式下，用当前的均值和方差做标准化 X_hat = (X - mean) / torch.sqrt(var + eps) # 更新移动平均的均值和方差 moving_mean = momentum * moving_mean + (1.0 - momentum) * mean moving_var = momentum * moving_var + (1.0 - momentum) * var Y = gamma * X_hat + beta # 缩放和移位 return Y, moving_mean.data, moving_var.data class BatchNorm(nn.Module): # num_features：完全连接层的输出数量或卷积层的输出通道数。 # num_dims：2表示完全连接层，4表示卷积层 def __init__(self, num_features, num_dims): super().__init__() if num_dims == 2: shape = (1, num_features) else: shape = (1, num_features, 1, 1) # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0 self.gamma = nn.Parameter(torch.ones(shape)) self.beta = nn.Parameter(torch.zeros(shape)) # 非模型参数的变量初始化为0和1 self.moving_mean = torch.zeros(shape) self.moving_var = torch.ones(shape) def forward(self, X): # 如果X不在内存上，将moving_mean和moving_var # 复制到X所在显存上 if self.moving_mean.device != X.device: self.moving_mean = self.moving_mean.to(X.device) self.moving_var = self.moving_var.to(X.device) # 保存更新过的moving_mean和moving_var Y, self.moving_mean, self.moving_var = batch_norm( X, self.gamma, self.beta, self.moving_mean, self.moving_var, eps=1e-5, momentum=0.9) return Y net = nn.Sequential( nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(), nn.Linear(16*4*4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(), nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(), nn.Linear(84, 10)) lr, num_epochs, batch_size = 1.0, 10, 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu()) net[1].gamma.reshape((-1,)), net[1].beta.reshape((-1,)) # (tensor([0.4863, 2.8573, 2.3190, 4.3188, 3.8588, 1.7942], device=\u0026#39;cuda:0\u0026#39;, # grad_fn=\u0026lt;ReshapeAliasBackward0\u0026gt;), # tensor([-0.0124, 1.4839, -1.7753, 2.3564, -3.8801, -2.1589], device=\u0026#39;cuda:0\u0026#39;, # grad_fn=\u0026lt;ReshapeAliasBackward0\u0026gt;)) 简洁实现 1 2 3 4 5 6 7 8 9 10 net = nn.Sequential( nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6), nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16), nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(), nn.Linear(256, 120), nn.BatchNorm1d(120), nn.Sigmoid(), nn.Linear(120, 84), nn.BatchNorm1d(84), nn.Sigmoid(), nn.Linear(84, 10)) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu()) 残差网络 ResNet 由 VGG 更改而来\n计算底层梯度时可以直接通过右侧 1*1 卷积层，因此底层也可以得到较大的梯度，这使得 ResNet 可以训练出 1000 层的模型\ng(x) = f(x) + x\n1*1 卷积层调整通道数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 import torch from torch import nn from torch.nn import functional as F from d2l import torch as d2l class Residual(nn.Module): # use_1x1conv: 是否使用1*1卷积层 def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1): super().__init__() self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strides) self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1) if use_1x1conv: self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides) else: self.conv3 = None self.bn1 = nn.BatchNorm2d(num_channels) self.bn2 = nn.BatchNorm2d(num_channels) def forward(self, X): Y = F.relu(self.bn1(self.conv1(X))) Y = self.bn2(self.conv2(Y)) if self.conv3: X = self.conv3(X) Y += X return F.relu(Y) b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) def resnet_block(input_channels, num_channels, num_residuals, first_block=False): blk = [] # 第一个块特殊处理 for i in range(num_residuals): if i == 0 and not first_block: # 通道数加倍，高宽减半 blk.append(Residual(input_channels, num_channels, use_1x1conv=True, strides=2)) else: blk.append(Residual(num_channels, num_channels)) return blk b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True)) b3 = nn.Sequential(*resnet_block(64, 128, 2)) b4 = nn.Sequential(*resnet_block(128, 256, 2)) b5 = nn.Sequential(*resnet_block(256, 512, 2)) net = nn.Sequential(b1, b2, b3, b4, b5, nn.AdaptiveAvgPool2d((1,1)), nn.Flatten(), nn.Linear(512, 10)) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 X = torch.rand(size=(1, 1, 224, 224)) for layer in net: X = layer(X) print(layer.__class__.__name__,\u0026#39;output shape:\\t\u0026#39;, X.shape) # Sequential output shape:\ttorch.Size([1, 64, 56, 56]) # Sequential output shape:\ttorch.Size([1, 64, 56, 56]) # Sequential output shape:\ttorch.Size([1, 128, 28, 28]) # Sequential output shape:\ttorch.Size([1, 256, 14, 14]) # Sequential output shape:\ttorch.Size([1, 512, 7, 7]) # AdaptiveAvgPool2d output shape:\ttorch.Size([1, 512, 1, 1]) # Flatten output shape:\ttorch.Size([1, 512]) # Linear output shape:\ttorch.Size([1, 10]) lr, num_epochs, batch_size = 0.05, 10, 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu()) ","date":"2024-09-12T00:00:00Z","permalink":"https://changle-liu.github.io/p/%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","title":"现代卷积神经网络-动手学深度学习"},{"content":"动手学深度学习v2\n课程链接：https://courses.d2l.ai/zh-v2/\n从全连接层到卷积 不变性 平移不变性：和检测对象的位置无关 局部性：只考虑检测对象的局部区域特征 重新考察全连接层 输入和输出变形为矩阵 权重变为4维张量 v：w的重新索引：k = i + a, l = j + b 平移不变性 =\u0026gt; v 不依赖于 i 和 j\n局部性 =\u0026gt; 不必使用远离$ x_{i, j} $的参数\n卷积层 互相关运算 互相关运算：cross-correlation\n交叉相关和卷积的区别 由上可实现：\n1 2 3 4 5 6 7 8 def corr2d(X, K): #@save \u0026#34;\u0026#34;\u0026#34;计算二维互相关运算\u0026#34;\u0026#34;\u0026#34; h, w = K.shape Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)) for i in range(Y.shape[0]): # 遍历并取出和K相同大小的块 for j in range(Y.shape[1]): Y[i, j] = (X[i:i + h, j:j + w] * K).sum() return Y 卷积层 在前向传播函数中直接调用编写好的互相关运算\n1 2 3 4 5 6 7 8 def corr2d(X, K): #@save \u0026#34;\u0026#34;\u0026#34;计算二维互相关运算\u0026#34;\u0026#34;\u0026#34; h, w = K.shape Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)) for i in range(Y.shape[0]): for j in range(Y.shape[1]): Y[i, j] = (X[i:i + h, j:j + w] * K).sum() return Y 目标边缘检测 简单应用：检测图像中不同颜色的边缘\n现有如下黑白图像（0黑1白）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 K = torch.tensor([[1.0, -1.0]]) Y = corr2d(X, K) Y # 输出： # tensor([[ 0., 1., 0., 0., 0., -1., 0.], # [ 0., 1., 0., 0., 0., -1., 0.], # [ 0., 1., 0., 0., 0., -1., 0.], # [ 0., 1., 0., 0., 0., -1., 0.], # [ 0., 1., 0., 0., 0., -1., 0.], # [ 0., 1., 0., 0., 0., -1., 0.]]) # 1表示白变黑，-1表示黑变白 # 将输入转置会发现检测不到 corr2d(X.t(), K) # tensor([[0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.]]) # 证明了K只能检测到竖直边缘 学习卷积核 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核 conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False) # 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度）， # 其中批量大小和通道数都为1 X = X.reshape((1, 1, 6, 8)) Y = Y.reshape((1, 1, 6, 7)) lr = 3e-2 # 学习率 for i in range(10): Y_hat = conv2d(X) l = (Y_hat - Y) ** 2 conv2d.zero_grad() l.sum().backward() # 迭代卷积核 conv2d.weight.data[:] -= lr * conv2d.weight.grad if (i + 1) % 2 == 0: print(f\u0026#39;epoch {i+1}, loss {l.sum():.3f}\u0026#39;) # epoch 2, loss 6.422 # epoch 4, loss 1.225 # epoch 6, loss 0.266 # epoch 8, loss 0.070 # epoch 10, loss 0.022 conv2d.weight.data.reshape((1, 2)) # tensor([[ 1.0010, -0.9739]]) 填充和步幅 填充 在输入图像的周围添加额外的行/列\n填充$ p_h $行和$ p_w $列，填充后输出的形状为\n（很少使用偶数核）\n这样取值使得输出形状变为$ n_h * n_w $\n步幅 每次滑动窗口时移动对应步幅数\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import torch from torch import nn # 为了方便起见，我们定义了一个计算卷积层的函数。 # 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数 def comp_conv2d(conv2d, X): # 这里的（1，1）表示批量大小和通道数都是1 X = X.reshape((1, 1) + X.shape) Y = conv2d(X) # 省略前两个维度：批量大小和通道 return Y.reshape(Y.shape[2:]) # 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列 conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1) X = torch.rand(size=(8, 8)) comp_conv2d(conv2d, X).shape # torch.Size([8, 8]) # 步幅设置为2 conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2) comp_conv2d(conv2d, X).shape # torch.Size([4, 4]) conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4)) comp_conv2d(conv2d, X).shape # torch.Size([2, 2]) 多输入多输出通道 彩色图片 = RGB三通道\n多输入通道 每个通道均执行互相关操作，然后将结果相加\n多输出通道 多个三位卷积核，每个核生成自己的输出通道，每个输出通道可以识别特定模式\n1x1卷积层 $ k_h = k_w = 1 $\n不识别空间模式，只把通道进行加权融合\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 import torch from d2l import torch as d2l # 定义-多输入 def corr2d_multi_in(X, K): # X和K均为3D # 先遍历“X”和“K”的第0个维度（通道维度），再把它们加在一起 # zip()对最外维度，即输入通道维度做遍历 return sum(d2l.corr2d(x, k) for x, k in zip(X, K)) X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]], [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]]) K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]]) corr2d_multi_in(X, K) # tensor([[ 56., 72.], # [104., 120.]]) # 定义-多输出 def corr2d_multi_in_out(X, K): # 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。 # 最后将所有结果在0维度上叠加在一起 return torch.stack([corr2d_multi_in(X, k) for k in K], 0) K = torch.stack((K, K + 1, K + 2), 0) # 在0维度上叠加 K.shape # torch.Size([3, 2, 2, 2]) corr2d_multi_in_out(X, K) # tensor([[[ 56., 72.], # [104., 120.]], # [[ 76., 100.], # [148., 172.]], # [[ 96., 128.], # [192., 224.]]]) # 1x1卷积 def corr2d_multi_in_out_1x1(X, K): c_i, h, w = X.shape c_o = K.shape[0] X = X.reshape((c_i, h * w)) K = K.reshape((c_o, c_i)) # 全连接层中的矩阵乘法 Y = torch.matmul(K, X) return Y.reshape((c_o, h, w)) X = torch.normal(0, 1, (3, 3, 3)) # X: 3*3*3大小 K = torch.normal(0, 1, (2, 3, 1, 1)) # K: 2输入 3输出 1*1大小 Y1 = corr2d_multi_in_out_1x1(X, K) Y2 = corr2d_multi_in_out(X, K) assert float(torch.abs(Y1 - Y2).sum()) \u0026lt; 1e-6 池化层 池化层/汇聚层 Pooling：降低卷积层对位置的敏感性\n二维最大池化 将滑动窗口中的最大值输出\n和卷积层一样，具有填充和步幅 没有可学习参数 不会融合多个通道，输入通道数 = 输出通道数 平均池化层 将.max()替换为.mean()\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 import torch from torch import nn from d2l import torch as d2l def pool2d(X, pool_size, mode=\u0026#39;max\u0026#39;): p_h, p_w = pool_size Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1)) # 创建输出 for i in range(Y.shape[0]): for j in range(Y.shape[1]): if mode == \u0026#39;max\u0026#39;: Y[i, j] = X[i: i + p_h, j: j + p_w].max() elif mode == \u0026#39;avg\u0026#39;: Y[i, j] = X[i: i + p_h, j: j + p_w].mean() return Y # 验证结果 X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]]) pool2d(X, (2, 2)) # tensor([[4., 5.], # [7., 8.]]) # 填充和步幅设定 X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4)) # tensor([[[[ 0., 1., 2., 3.], # [ 4., 5., 6., 7.], # [ 8., 9., 10., 11.], # [12., 13., 14., 15.]]]]) pool2d = nn.MaxPool2d(3, padding=1, stride=2) pool2d(X) # tensor([[[[ 5., 7.], # [13., 15.]]]]) # 设定任意大小窗口 pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1)) pool2d(X) # tensor([[[[ 5., 7.], # [13., 15.]]]]) # 多通道 X = torch.cat((X, X + 1), 1) X # tensor([[[[ 0., 1., 2., 3.], # [ 4., 5., 6., 7.], # [ 8., 9., 10., 11.], # [12., 13., 14., 15.]], # [[ 1., 2., 3., 4.], # [ 5., 6., 7., 8.], # [ 9., 10., 11., 12.], # [13., 14., 15., 16.]]]]) pool2d = nn.MaxPool2d(3, padding=1, stride=2) pool2d(X) # tensor([[[[ 5., 7.], # [13., 15.]], # [[ 6., 8.], # [14., 16.]]]]) LeNet 卷积层学习图片空间信息 全连接层转换到类别空间 1 2 3 4 5 6 7 8 9 10 11 12 13 import torch from torch import nn from d2l import torch as d2l net = nn.Sequential( nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(), nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(), nn.Linear(120, 84), nn.Sigmoid(), nn.Linear(84, 10)) ","date":"2024-09-10T00:00:00Z","permalink":"https://changle-liu.github.io/p/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","title":"卷积神经网络-动手学深度学习"},{"content":"动手学深度学习v2\n课程链接：https://courses.d2l.ai/zh-v2/\n层和块 自定义块 自定义MLP块，继承于nn.Module类，只需重写构造函数和前向传播函数\n1 2 3 4 5 6 7 8 9 10 11 12 class MLP(nn.Module): # 用模型参数声明层。这里，我们声明两个全连接的层 def __init__(self): # 调用MLP的父类Module的构造函数来执行必要的初始化。 super().__init__() self.hidden = nn.Linear(20, 256) # 隐藏层 self.out = nn.Linear(256, 10) # 输出层 # 定义模型的前向传播，即如何根据输入X返回所需的模型输出 def forward(self, X): # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。 return self.out(F.relu(self.hidden(X))) 1 2 net = MLP() net(X) 顺序块 1 2 3 4 5 6 7 8 9 10 11 12 class MySequential(nn.Module): def __init__(self, *args): super().__init__() # 按顺序插入args中的每一个类 for idx, module in enumerate(args): self._modules[str(idx)] = module def forward(self, X): # 按添加顺序遍历 for block in self._modules.values(): X = block(X) return X 在前向传播函数中执行代码 可以灵活将块进行组合\n1 2 3 4 5 6 7 8 9 10 11 12 13 class FixedHiddenMLP(nn.Module): def __init__(self): super().__init__() self.rand_weight = torch.rand((20, 20), requires_grad=False) self.linear = nn.Linear(20, 20) def forward(self, X): X = self.linear(X) X = F.relu(torch.mm(X, self.rand_weight) + 1) X = self.linear(X) while X.abs().sum() \u0026gt; 1: X /= 2 return X.sum() 参数管理 参数访问 定义一个单隐藏层的多层感知机\n1 2 3 4 5 6 7 import torch from torch import nn # 定义一个单隐藏层的多层感知机 net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1)) X = torch.rand(size=(2, 4)) net(X) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # net[2]可以获取第2层 print(net[2].state_dict()) # OrderedDict([(\u0026#39;weight\u0026#39;, tensor([[-0.0427, -0.2939, -0.1894, 0.0220, -0.1709, -0.1522, -0.0334, -0.2263]])), # (\u0026#39;bias\u0026#39;, tensor([0.0887]))]) # 访问目标参数 print(type(net[2].bias)) print(net[2].bias) print(net[2].bias.data) # \u0026lt;class \u0026#39;torch.nn.parameter.Parameter\u0026#39;\u0026gt; # Parameter定义可优化的参数 # Parameter containing: # tensor([0.0887], requires_grad=True) # tensor([0.0887]) # 访问梯度 net[2].weight.grad == None # 尚未调用反向传播，因此梯度为None # True # 访问所有参数 print(*[(name, param.shape) for name, param in net[0].named_parameters()]) print(*[(name, param.shape) for name, param in net.named_parameters()]) # (\u0026#39;weight\u0026#39;, torch.Size([8, 4])) (\u0026#39;bias\u0026#39;, torch.Size([8])) # (\u0026#39;0.weight\u0026#39;, torch.Size([8, 4])) (\u0026#39;0.bias\u0026#39;, torch.Size([8])) (\u0026#39;2.weight\u0026#39;, torch.Size([1, 8])) (\u0026#39;2.bias\u0026#39;, torch.Size([1])) # 通过name进行访问 net.state_dict()[\u0026#39;2.bias\u0026#39;].data # tensor([0.0887]) 嵌套块收集参数\nblock1在block2中出现4次 1 2 3 4 5 6 7 8 9 10 11 12 13 def block1(): return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU()) def block2(): net = nn.Sequential() for i in range(4): # 在这里嵌套 net.add_module(f\u0026#39;block {i}\u0026#39;, block1()) return net rgnet = nn.Sequential(block2(), nn.Linear(4, 1)) rgnet(X) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 print(rgnet) # Sequential( # (0): Sequential( # (block 0): Sequential( # (0): Linear(in_features=4, out_features=8, bias=True) # (1): ReLU() # (2): Linear(in_features=8, out_features=4, bias=True) # (3): ReLU() # ) # (block 1): Sequential( # (0): Linear(in_features=4, out_features=8, bias=True) # (1): ReLU() # (2): Linear(in_features=8, out_features=4, bias=True) # (3): ReLU() # ) # (block 2): Sequential( # (0): Linear(in_features=4, out_features=8, bias=True) # (1): ReLU() # (2): Linear(in_features=8, out_features=4, bias=True) # (3): ReLU() # ) # (block 3): Sequential( # (0): Linear(in_features=4, out_features=8, bias=True) # (1): ReLU() # (2): Linear(in_features=8, out_features=4, bias=True) # (3): ReLU() # ) # ) # (1): Linear(in_features=4, out_features=1, bias=True) # ) # 访问第一个主要的块中、第二个子块的第一层的偏置项 rgnet[0][1][0].bias.data # tensor([ 0.1999, -0.4073, -0.1200, -0.2033, -0.1573, 0.3546, -0.2141, -0.2483]) 参数初始化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # 初始为正态分布 def init_normal(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, mean=0, std=0.01) # _结尾的函数代表替换函数 nn.init.zeros_(m.bias) net.apply(init_normal) # .apply对net中的所有module进行操作 net[0].weight.data[0], net[0].bias.data[0] # 初始为常数（不建议） def init_constant(m): if type(m) == nn.Linear: nn.init.constant_(m.weight, 1) nn.init.zeros_(m.bias) net.apply(init_constant) net[0].weight.data[0], net[0].bias.data[0] # 对不同块应用不同初始化方法 def init_xavier(m): if type(m) == nn.Linear: nn.init.xavier_uniform_(m.weight) def init_42(m): if type(m) == nn.Linear: nn.init.constant_(m.weight, 42) net[0].apply(init_xavier) net[2].apply(init_42) print(net[0].weight.data[0]) print(net[2].weight.data) 自定义初始化\n例：对以下分布自定义初始化方法\n1 2 3 4 5 6 7 8 9 def my_init(m): if type(m) == nn.Linear: print(\u0026#34;Init\u0026#34;, *[(name, param.shape) for name, param in m.named_parameters()][0]) nn.init.uniform_(m.weight, -10, 10) m.weight.data *= m.weight.data.abs() \u0026gt;= 5 net.apply(my_init) net[0].weight[:2] 也可以直接设置参数\n1 2 3 net[0].weight.data[:] += 1 net[0].weight.data[0, 0] = 42 net[0].weight.data[0] 参数绑定 在多个层共享参数\n1 2 3 4 5 6 7 8 9 10 11 12 # 设置共享层的名称 shared = nn.Linear(8, 8) net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared, nn.ReLU(), nn.Linear(8, 1)) net(X) # 检查参数是否相同 print(net[2].weight.data[0] == net[4].weight.data[0]) net[2].weight.data[0, 0] = 100 # 确保它们实际上是同一个对象，而不只是有相同的值 print(net[2].weight.data[0] == net[4].weight.data[0]) 自定义层 不带参数的层 和块的构造比较类似\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import torch import torch.nn.functional as F from torch import nn class CenteredLayer(nn.Module): def __init__(self): super().__init__() def forward(self, X): return X - X.mean() # 使用 net = nn.Sequential(nn.Linear(8, 128), CenteredLayer()) 带参数的层 1 2 3 4 5 6 7 8 9 10 11 12 # 定义带参数的层 # in_units: 输入数 # units: 输出数 class MyLinear(nn.Module): def __init__(self, in_units, units): super().__init__() # 构造参数要放入nn.Parameter() self.weight = nn.Parameter(torch.randn(in_units, units)) self.bias = nn.Parameter(torch.randn(units,)) def forward(self, X): linear = torch.matmul(X, self.weight.data) + self.bias.data return F.relu(linear) 读写文件 sl张量 通过名称保存和加载\n1 2 3 4 5 6 7 8 9 import torch from torch import nn from torch.nn import functional as F x = torch.arange(4) torch.save(x, \u0026#39;x-file\u0026#39;) x2 = torch.load(\u0026#39;x-file\u0026#39;) x2 映射到张量的字典\n1 2 3 4 mydict = {\u0026#39;x\u0026#39;: x, \u0026#39;y\u0026#39;: y} torch.save(mydict, \u0026#39;mydict\u0026#39;) mydict2 = torch.load(\u0026#39;mydict\u0026#39;) mydict2 sl模型参数 1 2 3 4 5 6 7 8 9 10 11 12 class MLP(nn.Module): def __init__(self): super().__init__() self.hidden = nn.Linear(20, 256) self.output = nn.Linear(256, 10) def forward(self, x): return self.output(F.relu(self.hidden(x))) net = MLP() X = torch.randn(size=(2, 20)) Y = net(X) 将模型参数存储在\u0026rsquo;mlp.params\u0026rsquo;中\n1 torch.save(net.state_dict(), \u0026#39;mlp.params\u0026#39;) 1 2 3 4 5 6 7 8 clone = MLP() clone.load_state_dict(torch.load(\u0026#39;mlp.params\u0026#39;)) clone.eval() # 进入评估模式，不计算梯度 Y_clone = clone(X) Y_clone == Y # tensor([[True, True, True, True, True, True, True, True, True, True], # [True, True, True, True, True, True, True, True, True, True]]) GPU 17 使用和购买 GPU【动手学深度学习v2】_哔哩哔哩_bilibili\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 查询可用gpu的数量 torch.cuda.device_count() # 张量默认创建在CPU上 x = torch.tensor([1, 2, 3]) x.device # device(type=\u0026#39;cpu\u0026#39;) # 存储在GPU上 X = torch.ones(2, 3, device=try_gpu()) X # tensor([[1., 1., 1.], # [1., 1., 1.]], device=\u0026#39;cuda:0\u0026#39;) ","date":"2024-09-08T00:00:00Z","permalink":"https://changle-liu.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","title":"深度学习计算-动手学深度学习"},{"content":"动手学深度学习v2\n课程链接：https://courses.d2l.ai/zh-v2/\n多层感知机 感知机 线性回归：输出实数\nSoftmax：输出各分类概率\n感知机：二分类\n等价于使用批量大小为1的梯度下降\n损失函数：\n不能拟合XOR函数，只能产生线性分割面\n多层感知机 XOR学习 两个分类器进行组合\n模型 单隐藏层 $ \\sigma $的加入使得多层感知机不会退化为线性模型\n多类分类 通过softmax处理\n多隐藏层 超参数：\n隐藏层数 每层隐藏层大小 激活函数 代码实现 依旧采用Fashion-MNIST图像分类数据集\n手动实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # 实现单隐藏层的多层感知机，隐藏单元为256个 num_inputs, num_outputs, num_hiddens = 784, 10, 256 # W1和W2乘0.01使得方差为0.01 W1 = nn.Parameter(torch.randn( num_inputs, num_hiddens, requires_grad=True) * 0.01) b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True)) W2 = nn.Parameter(torch.randn( num_hiddens, num_outputs, requires_grad=True) * 0.01) b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True)) params = [W1, b1, W2, b2] # 实现ReLU def relu(X): a = torch.zeros_like(X) return torch.max(X, a) # 模型 def net(X): X = X.reshape((-1, num_inputs)) H = relu(X@W1 + b1) # 这里“@”代表矩阵乘法 return (H@W2 + b2) # 损失 loss = nn.CrossEntropyLoss(reduction=\u0026#39;none\u0026#39;) # 训练 num_epochs, lr = 10, 0.1 updater = torch.optim.SGD(params, lr=lr) d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater) 简洁实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 模型 net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), nn.Linear(256, 10)) def init_weights(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, std=0.01) net.apply(init_weights); batch_size, lr, num_epochs = 256, 0.1, 10 loss = nn.CrossEntropyLoss(reduction=\u0026#39;none\u0026#39;) trainer = torch.optim.SGD(net.parameters(), lr=lr) train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer) 模型选择、欠拟合和过拟合 模型选择 误差 训练误差：模型在训练数据上的误差 泛化误差：新数据上的误差 我们更关心泛化误差\n验证数据集和测试数据集 验证数据集：评估模型好坏（不跟训练数据混在一起） 测试数据集：只用一次、不可用来调参 K-则交叉验证 没有足够多数据时使用\n算法：\n数据分成K块（K常取5或10） for i = 1, \u0026hellip;, K，使用第 i 块作为验证数据集，其余用来训练 取K个验证集误差的平均 过拟合和欠拟合 数据简单 数据复杂 模型容量低 正常 欠拟合 模型容量高 过拟合 正常 模型容量 模型容量：拟合各种函数的能力 低容量：难以拟合训练数据 高容量：会记住所有训练数据 估计模型容量： 不同种类算法间难以比较 主要因素： 参数个数 参数值选择范围 VC维（了解） 深度学习中衡量不准确\n权重衰退 weight-decay 使用均方范数作为硬性限制 限制偏移b没有明显效果 较小的$ \\theta $意味着更强的正则项 正则项（Regularization term）是机器学习和统计模型中用于防止模型过拟合（overfitting）的一种技术。它通过在损失函数（loss function）中加入一个额外的惩罚项，来约束模型的复杂度，迫使模型在训练时选择较为简单的解，以避免在训练集上表现得过好但在测试集上泛化能力差的问题。\n正则项仅在训练过程中使用\n使用均方范数作为柔性限制 对任意$ \\theta $存在$ \\lambda $使得之前的目标函数等于：\n超参数$ \\lambda $代表了正则项的重要程度\n$ \\lambda = 0 $：无作用 惩罚项的加入使得最优解向原点偏移\n梯度： w随时间t更新： 通常衰退$ \\eta\\lambda \u0026lt; 1 $，因此称作权重衰退\n代码实现 从零实现 根据公式生成数据：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 %matplotlib inline import torch from torch import nn from d2l import torch as d2l # 生成数据 n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5 true_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05 train_data = d2l.synthetic_data(true_w, true_b, n_train) train_iter = d2l.load_array(train_data, batch_size) test_data = d2l.synthetic_data(true_w, true_b, n_test) test_iter = d2l.load_array(test_data, batch_size, is_train=False) # 初始化参数 def init_params(): w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True) b = torch.zeros(1, requires_grad=True) return [w, b] # 定义L2范数惩罚 def l2_penalty(w): return torch.sum(w.pow(2)) / 2 # 训练 def train(lambd): w, b = init_params() net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss num_epochs, lr = 100, 0.003 animator = d2l.Animator(xlabel=\u0026#39;epochs\u0026#39;, ylabel=\u0026#39;loss\u0026#39;, yscale=\u0026#39;log\u0026#39;, xlim=[5, num_epochs], legend=[\u0026#39;train\u0026#39;, \u0026#39;test\u0026#39;]) for epoch in range(num_epochs): for X, y in train_iter: # 增加了L2范数惩罚项， # 广播机制使l2_penalty(w)成为一个长度为batch_size的向量 l = loss(net(X), y) + lambd * l2_penalty(w) l.sum().backward() d2l.sgd([w, b], lr, batch_size) if (epoch + 1) % 5 == 0: animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss), d2l.evaluate_loss(net, test_iter, loss))) print(\u0026#39;w的L2范数是：\u0026#39;, torch.norm(w).item()) # 分别取lambd = 0, 3 train(lambd=0) train(lambd=3) 运行结果：\n$ \\lambda $ 0 3 w loss 简洁实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def train_concise(wd): net = nn.Sequential(nn.Linear(num_inputs, 1)) for param in net.parameters(): param.data.normal_() loss = nn.MSELoss(reduction=\u0026#39;none\u0026#39;) num_epochs, lr = 100, 0.003 # 偏置参数没有衰减 trainer = torch.optim.SGD([ {\u0026#34;params\u0026#34;:net[0].weight,\u0026#39;weight_decay\u0026#39;: wd}, #此处设置wd {\u0026#34;params\u0026#34;:net[0].bias}], lr=lr) animator = d2l.Animator(xlabel=\u0026#39;epochs\u0026#39;, ylabel=\u0026#39;loss\u0026#39;, yscale=\u0026#39;log\u0026#39;, xlim=[5, num_epochs], legend=[\u0026#39;train\u0026#39;, \u0026#39;test\u0026#39;]) for epoch in range(num_epochs): for X, y in train_iter: trainer.zero_grad() l = loss(net(X), y) l.mean().backward() trainer.step() if (epoch + 1) % 5 == 0: animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss), d2l.evaluate_loss(net, test_iter, loss))) print(\u0026#39;w的L2范数：\u0026#39;, net[0].weight.norm().item()) 丢弃法 dropout 原理 好的模型需要对输入数据的扰动鲁棒，考虑在层之间加入噪音\n无偏差加入噪音：\n加入噪音前后的期望不变：E[x\u0026rsquo;] = x $ x = 0 * p + (1 - p) * x / (1 - p) $\n丢弃法通常作用在隐藏全连接层的输出上，将一些输出项随机变成0 训练：\n推理：\n推理过程中不使用正则项，丢弃法直接返回输出\nh = dropout(h)\n代码实现 从零实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torch from torch import nn from d2l import torch as d2l # 实现以dropout的概率丢弃X中的元素 def dropout_layer(X, dropout): assert 0 \u0026lt;= dropout \u0026lt;= 1 # 在本情况中，所有元素都被丢弃 if dropout == 1: return torch.zeros_like(X) # 在本情况中，所有元素都被保留 if dropout == 0: return X mask = (torch.rand(X.shape) \u0026gt; dropout).float() return mask * X / (1.0 - dropout) # 直接做乘法运算比频繁读取的运行速度快 测试：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 两个隐藏层的多层感知机，每层256个单元 num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256 # 定义模型 dropout1, dropout2 = 0.2, 0.5 class Net(nn.Module): def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2, is_training = True): super(Net, self).__init__() self.num_inputs = num_inputs self.training = is_training self.lin1 = nn.Linear(num_inputs, num_hiddens1) self.lin2 = nn.Linear(num_hiddens1, num_hiddens2) self.lin3 = nn.Linear(num_hiddens2, num_outputs) self.relu = nn.ReLU() def forward(self, X): H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs)))) # 只有在训练模型时才使用dropout if self.training == True: # 在第一个全连接层之后添加一个dropout层 H1 = dropout_layer(H1, dropout1) H2 = self.relu(self.lin2(H1)) if self.training == True: # 在第二个全连接层之后添加一个dropout层 H2 = dropout_layer(H2, dropout2) out = self.lin3(H2) return out net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2) 训练结果：\n简洁实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), # 在第一个全连接层之后添加一个dropout层 nn.Dropout(dropout1), nn.Linear(256, 256), nn.ReLU(), # 在第二个全连接层之后添加一个dropout层 nn.Dropout(dropout2), nn.Linear(256, 10)) def init_weights(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, std=0.01) net.apply(init_weights); 数值稳定性 考虑一个d层的神经网络\n则$ l $关于$ \\textbf{W}_t $的梯度为：\n梯度爆炸 例：MLP\n使用ReLU作为激活函数\n则梯度是Wi中值为1的项做乘法。当d-t较大时，梯度将会非常大\n带来问题：\n值超出值域（对16位浮点数来说尤为严重：6e-5 - 6e4） 对学习率敏感 过大：梯度更大 过小：训练无进展 梯度消失 sigmoid作为激活函数\n则梯度为d-t个小数值的乘积\n带来问题：\n梯度值变成0 无论如何选择学习率，训练无进展 限制了神经网络的深度：对较深的神经网络，反向梯度计算使得底层训练效果不好 数值稳定 如何让梯度值范围合理？ 乘法变加法：ResNet、LSTM 梯度归一化、梯度剪裁 合理的权重初始化和激活函数 我们希望每层的输出和梯度的均值和方差保持一致\n在参数初始化时，最优解附近的表面更加平缓，较远处更容易数值不稳定。因此需要合理的初始化参数\n对于正向情况，h与w独立，若使得期望为0，方差为$ \\gamma t $，则有$ n{t-1}\\gamma_t = 1 $\n同样，对于反向情况，有$ n_{t}\\gamma_t = 1 $\n推导：模型初始化和激活函数_哔哩哔哩_bilibili\nXavier初始 $ (n_{t-1}\\gamma_t + n_{t}\\gamma_t)/2 = 1 $\n则$ \\gamma_t = 2/(n_{t-1}+n_t) $\n则参数初始化需满足：\n激活函数 假设激活函数为线性\n反向的到相同结果，即 f(x) = x\n将现有激活函数调整为满足零点附近近似 f(x) = x\n","date":"2024-09-05T00:00:00Z","permalink":"https://changle-liu.github.io/p/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","title":"多层感知机-动手学深度学习"},{"content":"动手学深度学习v2\n课程链接：https://courses.d2l.ai/zh-v2/\n线性回归 $ y = \u0026lt;\\mathbf{w}, \\mathbf{x}\u0026gt; + b $\n有显示解 可看作单层神经网络 损失函数 基础优化算法 梯度下降\n初始值w0 重复迭代t = 1, 2, 3 \u0026hellip; $ \\eta $学习率：步长的超参数\n从零实现 1 2 3 4 5 6 7 8 9 10 11 # 生成数据集，其中w = [2, -3.4], b = 4.2，噪声成标准差为1的正态分布 def synthetic_data(w, b, num_examples): #@save \u0026#34;\u0026#34;\u0026#34;生成y=Xw+b+噪声\u0026#34;\u0026#34;\u0026#34; X = torch.normal(0, 1, (num_examples, len(w))) y = torch.matmul(X, w) + b y += torch.normal(0, 0.01, y.shape) return X, y.reshape((-1, 1)) true_w = torch.tensor([2, -3.4]) true_b = 4.2 features, labels = synthetic_data(true_w, true_b, 1000) 作图可观察到线性关系\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 生成大小为batch_size的小批量 def data_iter(batch_size, features, labels): num_examples = len(features) indices = list(range(num_examples)) # 这些样本是随机读取的，没有特定的顺序 random.shuffle(indices) for i in range(0, num_examples, batch_size): batch_indices = torch.tensor( indices[i: min(i + batch_size, num_examples)]) yield features[batch_indices], labels[batch_indices] # 初始化参数 w = torch.normal(0, 0.01, size=(2,1), requires_grad=True) b = torch.zeros(1, requires_grad=True) # 定义模型 def linreg(X, w, b): #@save \u0026#34;\u0026#34;\u0026#34;线性回归模型\u0026#34;\u0026#34;\u0026#34; return torch.matmul(X, w) + b # 定义损失函数 def squared_loss(y_hat, y): #@save \u0026#34;\u0026#34;\u0026#34;均方损失\u0026#34;\u0026#34;\u0026#34; return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2 # 定义优化算法 def sgd(params, lr, batch_size): #@save \u0026#34;\u0026#34;\u0026#34;小批量随机梯度下降\u0026#34;\u0026#34;\u0026#34; with torch.no_grad(): for param in params: param -= lr * param.grad / batch_size param.grad.zero_() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 lr = 0.03 num_epochs = 3 net = linreg loss = squared_loss for epoch in range(num_epochs): for X, y in data_iter(batch_size, features, labels): l = loss(net(X, w, b), y) # X和y的小批量损失 # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起， # 并以此计算关于[w,b]的梯度 l.sum().backward() sgd([w, b], lr, batch_size) # 使用参数的梯度更新参数 with torch.no_grad(): train_l = loss(net(features, w, b), labels) print(f\u0026#39;epoch {epoch + 1}, loss {float(train_l.mean()):f}\u0026#39;) # epoch 1, loss 0.037387 # epoch 2, loss 0.000139 # epoch 3, loss 0.000052 # 误差 print(f\u0026#39;w的估计误差: {true_w - w.reshape(true_w.shape)}\u0026#39;) print(f\u0026#39;b的估计误差: {true_b - b}\u0026#39;) # w的估计误差: tensor([ 0.0004, -0.0006], grad_fn=\u0026lt;SubBackward0\u0026gt;) # b的估计误差: tensor([0.0007], grad_fn=\u0026lt;RsubBackward1\u0026gt;) 简洁实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 # 读取数据集 # 调用torch.utils.data中现有api def load_array(data_arrays, batch_size, is_train=True): #@save \u0026#34;\u0026#34;\u0026#34;构造一个PyTorch数据迭代器\u0026#34;\u0026#34;\u0026#34; dataset = data.TensorDataset(*data_arrays) return data.DataLoader(dataset, batch_size, shuffle=is_train) batch_size = 10 data_iter = load_array((features, labels), batch_size) # 可使用next()验证迭代器是否正常工作 next(iter(data_iter)) # 定义模型 # Sequential()将多个层串联到一起 # Linear(2, 1)中两个参数分别为输入和输出特征形状 # nn是神经网络的缩写 from torch import nn net = nn.Sequential(nn.Linear(2, 1)) # 初始化模型参数 net[0].weight.data.normal_(0, 0.01) net[0].bias.data.fill_(0) # 定义损失函数 loss = nn.MSELoss() # 定义优化算法 trainer = torch.optim.SGD(net.parameters(), lr=0.03) # 训练 num_epochs = 3 for epoch in range(num_epochs): for X, y in data_iter: l = loss(net(X) ,y) trainer.zero_grad() l.backward() trainer.step() l = loss(net(features), labels) print(f\u0026#39;epoch {epoch + 1}, loss {l:f}\u0026#39;) # epoch 1, loss 0.000248 # epoch 2, loss 0.000103 # epoch 3, loss 0.000103 # 误差 w = net[0].weight.data print(\u0026#39;w的估计误差：\u0026#39;, true_w - w.reshape(true_w.shape)) b = net[0].bias.data print(\u0026#39;b的估计误差：\u0026#39;, true_b - b) # w的估计误差： tensor([-0.0010, -0.0003]) # b的估计误差： tensor([-0.0003]) softmax回归 Softmax回归 对类别进行一位有效编码 使用均方损失训练 输出匹配概率 y和y_hat的区别作为损失 交叉熵损失 利用交叉熵来衡量两个概率的区别\n损失即为\n梯度为真实概率和预测概率的区别\n从零实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 import torch from IPython import display from d2l import torch as d2l # 使用Fashion-MNIST数据集 batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) # 初始化参数 # 展平每个28*28图像，看作784向量，暂不考虑图像空间结构特征 # 输出维度 == 类别数 num_inputs = 784 num_outputs = 10 W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True) b = torch.zeros(num_outputs, requires_grad=True) # 定义softmax def softmax(X): X_exp = torch.exp(X) partition = X_exp.sum(1, keepdim=True) return X_exp / partition # 这里应用了广播机制 # 定义模型 def net(X): return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b) # 定义损失函数 # 通过正确的[range(len(y_hat)), y]索引选择所有需要的元素，不需要低效的for循环 def cross_entropy(y_hat, y): return - torch.log(y_hat[range(len(y_hat)), y]) # 分类精度 def accuracy(y_hat, y): #@save \u0026#34;\u0026#34;\u0026#34;计算预测正确的数量\u0026#34;\u0026#34;\u0026#34; if len(y_hat.shape) \u0026gt; 1 and y_hat.shape[1] \u0026gt; 1: y_hat = y_hat.argmax(axis=1) # .argmax(axis=1)用于获取轴1上最大元素的索引 cmp = y_hat.type(y.dtype) == y return float(cmp.type(y.dtype).sum()) # accuracy(y_hat, y) / len(y) 即求得分类精度 # 对数据迭代器求精度 def evaluate_accuracy(net, data_iter): #@save \u0026#34;\u0026#34;\u0026#34;计算在指定数据集上模型的精度\u0026#34;\u0026#34;\u0026#34; if isinstance(net, torch.nn.Module): net.eval() # 将模型设置为评估模式 metric = Accumulator(2) # 正确预测数、预测总数 with torch.no_grad(): for X, y in data_iter: metric.add(accuracy(net(X), y), y.numel()) return metric[0] / metric[1] 简洁实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import torch from torch import nn from d2l import torch as d2l batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) # 初始化参数 # 在Linear前加一个展平层，用于将图像调整为784 net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10)) def init_weights(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, std=0.01) net.apply(init_weights); # 交叉熵损失 # 这里的底层实现和传统softmax略有不同，避免了上溢和下溢的问题 loss = nn.CrossEntropyLoss(reduction=\u0026#39;none\u0026#39;) # 优化算法 trainer = torch.optim.SGD(net.parameters(), lr=0.1) # 训练 num_epochs = 10 d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer) ","date":"2024-09-04T00:00:00Z","permalink":"https://changle-liu.github.io/p/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BD%91%E7%BB%9C-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","title":"线性回归网络-动手学深度学习"},{"content":"动手学深度学习v2\n课程链接：https://courses.d2l.ai/zh-v2/\n数据操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import torch # 创建行向量 x = torch.arange(12) # 访问沿轴形状 x.shape # 元素总数 num element x.numel() # 改变张量形状 X = x.reshape(3, 4) # 创建特殊张量 torch.zeros((2, 3, 4)) torch.ones((2, 3, 4)) torch.randn(3, 4) torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) # 运算 x + y, x - y, x * y, x / y, x ** y X == Y X.sum() torch.exp(x) # 沿轴连接（轴0） torch.cat((X, Y), dim=0) # 与numpy转换 A = X.numpy() B = torch.tensor(A) # 转换py标量 a.item() float(a) 数据预处理 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \u0026#39;\u0026#39;\u0026#39; # 创建数据集 import os os.makedirs(os.path.join(\u0026#39;..\u0026#39;, \u0026#39;data\u0026#39;), exist_ok=True) data_file = os.path.join(\u0026#39;..\u0026#39;, \u0026#39;data\u0026#39;, \u0026#39;house_tiny.csv\u0026#39;) with open(data_file, \u0026#39;w\u0026#39;) as f: f.write(\u0026#39;NumRooms,Alley,Price\\n\u0026#39;) # 列名 f.write(\u0026#39;NA,Pave,127500\\n\u0026#39;) # 每行表示一个数据样本 f.write(\u0026#39;2,NA,106000\\n\u0026#39;) f.write(\u0026#39;4,NA,178100\\n\u0026#39;) f.write(\u0026#39;NA,NA,140000\\n\u0026#39;) \u0026#39;\u0026#39;\u0026#39; # 加载数据集 # !pip install pandas import pandas as pd data = pd.read_csv(data_file) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # 创建出的数据集 # NumRooms Alley Price # 0 NaN Pave 127500 # 1 2.0 NaN 106000 # 2 4.0 NaN 178100 # 3 NaN NaN 140000 # 利用iloc切分数据集 inputs = data.iloc[:, 0:2] outputs = data.iloc[:, 2] # 均值替换NaN inputs = inputs.fillna(inputs.mean(numeric_only=True)) # 离散值处理 inputs = pd.get_dummies(inputs, dummy_na=True) # 处理后的inputs # NumRooms Alley_Pave Alley_nan # 0 3.0 True False # 1 2.0 False True # 2 4.0 False True # 3 3.0 False True # 转换为张量 import torch X = torch.rensor(inputs.to_numpy(dtype=float)) # (tensor([[3., 1., 0.], # [2., 0., 1.], # [4., 0., 1.], # [3., 0., 1.]], dtype=torch.float64) 线代 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import torch # 标量 x = torch.tensor(3.0) # 向量 x = torch.arange(4) # 形状 len(x) x.shape # 矩阵 转置 A = torch.arange(20).reshape(5, 4) A.T # 张量 X = torch.arange(24).reshape(2, 3, 4) # Hadamard积 A * B # 降维沿轴求和 A_sum_axis0 = A.sum(axis=0) # 非降维求和 sum_A = A.sum(axis=1, keepdims=True) # 沿轴计算累计总和 A.cumsum(axis=0) # 点积 torch.dot(x, y) torch.sum(x*y) # 矩阵向量积 torch.mv(A, x) # 矩阵乘法 torch.(A, B) # L2范数 torch.norm(u) # L1范数需要手动求 torch.abs(u).sum() 微积分 亚导数 将导数扩展到不可导的地方，如y = |x| 的 x = 0 处\n梯度 将倒数拓展到向量\n自动求导 两种求导模式\n计算复杂度 内存复杂度 正向 O(n) O(n) （需要存储所有中间结果） 反向 O(n) O(1) 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 x = torch.arange(4.0, requires_grad=True) # x.requires_grad_(True) x.grad y = 2 * torch.dot(x, x) # 反向传播计算x每个分量的梯度，结果在x.grad中 y.backward() # PyTorch默认累计梯度 # 清除梯度值 x.grad_zero_() # 分离并记录张量 u = y.detach() # 自动微分同样可应用在py控制流中 ","date":"2024-09-03T00:00:00Z","permalink":"https://changle-liu.github.io/p/%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","title":"预备知识-动手学深度学习"}]